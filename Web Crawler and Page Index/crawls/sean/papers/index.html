<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en"> <head>
<script src="../scripts/fancyzoom/FancyZoom.js" type="text/javascript"></script>
<script src="../scripts/fancyzoom/FancyZoomHTML.js" type="text/javascript"></script>
<script src="../scripts/reveal.js" type="text/javascript"></script>
<link rel="stylesheet" href="../site.css" type="text/css">
<title>Sean Luke : Publications</title>

</head>

<body onload="setupZoom()">
<div class="outer">
  <table width="100%" class="outer">
      <tr>
      <td width="70%" class="main">
	<table width="100%" class="dl" cellspacing=0 cellpadding=0 border=0>
	<a name="Old"></a>
	  <tr><td class="dtlast">Bibliography</td>
	<td class="ddlast">
 	 <a href="luke.bib">Available in BibTe&#x03A7;.</a>
	</td>

	  <tr><td colspan=2 class="spread"><a name="Books"></a>Book</td>
	  </tr>


	<a name="Books"></a><tr><td class="dtlast">Essentials of Metaheuristics</td><td class="ddlast">
       <dl>
<!--         <dt><i>Author: </i><dd>Sean Luke -->
         <dt><i>Citation:</i><dd>Sean Luke.  2009.  <i>Essentials of Metaheuristics</i>.  Available for free at <a href="http://cs.gmu.edu/~sean/book/metaheuristics/">http://cs.gmu.edu/~sean/book/metaheuristics/</a>
	</dl>






    <tr><td colspan=2 class="spread"><a name="Simulation"></a>Multiagent Behavior and Simulation</td>
	  </tr>       


 <a name="ThrowingInTheTowelFaithlessBountyHuntersAsATaskAllocationMechanism"></a><tr><td class="dte">Throwing in the Towel: Faithless Bounty Hunters as a Task Allocation Mechanism</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/bounty16.pdf&quot;">PDF</a>
         <dt><i>Citation:</i><dd>Drew Wicke, Ermo Wei, and Sean Luke.  2016.  Throwing in the Towel: Faithless Bounty Hunters as a Task Allocation Mechanism. In <i>IJCAI workshop on Interactions with Mixed Agent Types</i>. 
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e75');">Show / Hide</a><dd id='e75' style="display:none;">
Bounty hunting has been shown to solve the multiagent task allocation problem robustly in noisy and dynamic scenarios with multiple other agents of unknown quality. This bounty hunting model does not require task exclusivity, but does require task commitment. We examine eliminating this second requirement, thus allowing bounty hunters to commit to tasks but abandon them later (to <i>jump ship</i>) for more promising tasks, without telling other agents of their intent. Bounty hunting agents must use an adaptive valuation mechanism in order to avoid too much redundancy in working on tasks. We examine how one might revise this mechanism in the face of task abandonment. We compare jumping ship favorably against other bounty hunting methods and against methods which are more "auction-like" in that they permit exclusivity, and we do so under both static environments and ones in which agents, and task scenarios, change dynamically.
</dl>



 <a name="AntGeometers"></a><tr><td class="dte">Ant Geometers</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/alife16-geometers.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Sean Luke, Katherine Russell, and Bryan Hoyle.  2016.  Ant Geometers. In <i>15th International Conference on the Synthesis and Simulation of Living Systems (ALIFE 2016).</i>
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e74');">Show / Hide</a><dd id='e74' style="display:none;">
Just how much can a pheromone-enabled swarm do? Motivated by robotic construction, we set out to show that a swarm of computationally simple ants, communicating only via pheromones, can in fact perform classic compass-straightedge geometry, and thus can make many shapes and perform many nontrivial geometric tasks. The ants do not need specially-designed stigmergic building materials, a prepared environment, local or global direct communication facilities (such as radio or line-of-sight signaling), or any localization beyond initial starting points for drawing. We describe the proof of concept in replicable detail. We then note that its accuracy and efficiency can be greatly improved through augmentation with a simple embeddable broadcast mechanism.
</dl>


 <a name="BountyHuntersAndMultiagentTaskAllocation"></a><tr><td class="dte">Bounty Hunters and Multiagent Task Allocation</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/bounty.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Drew Wicke, David Freelan, and Sean Luke. 2015. Bounty Hunters and Multiagent Task Allocation. In <i>International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2015).</i>
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e70');">Show / Hide</a><dd id='e70' style="display:none;">
We propose a system for multiagent task allocation inspired by the model used by bounty hunters and bail bondsmen. A bondsman posts tasks for agents to complete, along with bounties to be collected by an agent on completion. Multiple agents, taking the role of the bounty hunters, compete to finish tasks and collect their bounties. While a task remains uncompleted, its bounty gradually rises, making it more and more desirable to pursue. Unlike auctions, this model does not assume rationality in agents&rsquo; bids (as there are none), and since tasks are not exclusive to given agents, the system is robust to highly noisy environments. We examine how agents may locally develop rational task valuations in such an environment, gradually adapting to dividing tasks according to the agents best suited to them. We compare different methods for building these valuations against approaches which are more &ldquo;auction-like&rdquo; in that they permit exclusivity, and we do so under both static environments and ones in which agents, and task details, change dynamically.
</dl>


 <a name="CollaborativeForagingUsingBeacons"></a><tr><td class="dte">Collaborative Foraging using Beacons</td><td class="dde">
       <dl>
         <!--<dt><i>Authors: </i><dd>Gabriel Balan and Sean Luke  -->
         <!--<dt><i>Formats: </i><dd>-->
              <a href="https://cs.gmu.edu/~sean/papers/aamas10-beacons.pdf">PDF</a>
	<dt><i>Note:</i><dd>This is an updated version of the paper, with some typos and bugfixes corrected from the original AAMAS publication.
         <dt><i>Citation:</i><dd>Brian Hrolenok, Sean Luke, Keith Sullivan, and Christopher Vo. 2010. Collaborative Foraging using Beacons.  In <i>Proceedings of the Ninth International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2010).</i>  van der Hoek <i>et al</i>, eds.  1197&ndash;1204.

         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e7');">Show / Hide</a><dd id='e7' style="display:none;">
A classic example of multiagent coordination in a shared environment involves the use of pheromone deposits as a communication mechanism.  Due to physical limitations in deploying actual pheromones, we propose a sparse representation of the pheromones using movable beacons.  There is no communication between the beacons to propagate pheromones; instead, robots make movement and update decisions based entirely on local pheromone values.  Robots deploy the beacons throughout the environment, and subsequently move them and update them using a variation of value iteration.  Simulation results show that our approach is effective at finding good trails, locally improving them, and adapting to dynamic changes in the environments. 
</dl>

 <a name="CanYouDoMeAFavor"></a><tr><td class="dte">Can You Do Me A Favor?</td><td class="dde">
       <dl>
         <!--<dt><i>Authors: </i><dd>Gabriel Balan and Sean Luke  -->
         <!--<dt><i>Formats: </i><dd>-->
              <a href="https://cs.gmu.edu/~sean/papers/aamas10-favor.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Sean Luke, and Brian Hrolenok. 2010. Can You Do Me A Favor?  In <i>AAMAS 2010 Workshop on Trust in Agent Societies.</i>

         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e8');">Show / Hide</a><dd id='e8' style="display:none;">
Multiagent systems often require coordination among the agents to maximize system utility. Using the notion of favors, we propose a technique, <i>flexible reciprocal altruism</i>, which determines when one agent should grant a favor to another agent based on past interactions. The desired rate of altruism is controllable, and as a result the loss associated with granting unmatched favors is bounded and amortized over all past interactions. In flexible reciprocal altruism the desired acceptable loss is independent of the cost and value of the favors. Experiments show that our technique performs well with different cost/value tradeoffs, numbers of agents, and load.
 </dl>


	  
 <a name="HistoryBasedTrafficControl"></a><tr><td class="dte">History-based Traffic Control</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Gabriel Balan and Sean Luke  -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/aamas06-traffic.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Gabriel Balan and Sean Luke. 2006. History-based Traffic Control.  In <i>Proceedings of the Fifth International Conference on Autonomous Agents and Multi-Agent Systems</i>. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g5');">Show / Hide</a><dd id='g5' style="display:none;">What if traffic lights gave you a break after you've spent a long time waiting in traffic elsewhere? In this paper we examine a variety of multi-agent traffic light controllers which consider vehicles' past stopped-at-red histories. For example, a controller might distribute credits to cars as they wait and award the green light to lanes with the most credits, allowing cars to keep the credits they accumulate during travel. Such history-based controllers are intended to provide a kind of global fairness, reducing the variance in mean time spent waiting at lights during trips. We compare these controllers against other multi-agent controllers which only consider present information, and discover, among other things, that while the history-based controllers are among the most robust, they often unexpectedly provide more efficiency than fairness. 


       </dl>

 <a name="TunablyDecentralizedAlgorithmsForCooperativeTargetObservation"></a><tr><td class="dte">Tunably Decentralized Algorithms for Cooperative Target Observation</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke, Keith Sullivan, Liviu Panait, and Gabriel Balan -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/luke05tunable.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Keith Sullivan, Liviu Panait, and Gabriel Balan. 2005. Tunably Decentralized Algorithms for Cooperative Target Observation.  In <i>Proceedings of the 2005 Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</i>.  Pages 911-917.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g0');">Show / Hide</a><dd id='g0' style="display:none;">Multi-agent problem domains may require distributed algorithms for a variety of reasons: local sensors, limitations of communication, and availability of distributed computational resources. In the absence o f these constraints, centralized algorithms are often more efficient, simply because they are able to take advantage of more information. We introduce a variant of the cooperative target observation domain w which is free of such constraints. We propose two algorithms, inspired by K-means clustering and hill-climbing respectively, which are scalable in degree of decentralization. Neither algorithm consistently o outperforms the other across over all problem domain settings. Surprisingly, we find that hill-climbing is sensitive to degree of decentralization, while K-means is not. We also experiment with a combination n of the two algorithms which draws strength from each. 

       </dl>


 <a name="AgentBasedDynamicsOfSocialComplexityModelingAdaptiveBehaviorAndLongTermChangeInInnerAsia"></a><tr><td class="dte">Agent-based Dynamics of Social Complexity: Modeling Adaptive Behavior and Long-term Change in Inner Asia</td><td class="dde">
       <dl>
	 <!--<dt><i>Formats: </i><dd>-->
		<a href="https://cs.gmu.edu/~sean/papers/wcss06.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Claudio Cioffi-Revilla, Sean Luke, Dawn C. Parker, J. D. Rogers, W. W. Fitzhugh, W. Honeychurch, B. Frohlich, P. DePriest, and Naran Bazarsad. 2006. Agent-based Dynamics of Social Complexity: Modeling Adaptive Behavior and Long-term Change in Inner Asia.  In <i>Proceedings of the First World Congress on Social Simulation</i>. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g4');">Show / Hide</a><dd id='g4' style="display:none;">We present a new international project to develop temporally and spatially calibrated agent-based models of the rise and fall of polities in Inner Asia (Central Eurasia) in the past 5,000 years. Gaps in theory, data, and computational models for explaining long-term sociopolitical change--both growth and decay--motivate this project. We expect three contributions: (1) new theoretically grounded simulation models validated and calibrated by the best available data; (2) a new long-term cross-cultural database with several data sets; and (3) new conceptual, theoretical, and methodological contributions for understanding social complexity and long-term change and adaptation in real and artificial societies. Our theoretical framework is based on explaining sociopolitical evolution by the process of "canonical variation". 

       </dl>



       
	                      <a name="MASONAMultiAgentSimulationEnvironment"></a><tr><td class="dte">MASON: a Multi-agent Simulation Environment</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke, Claudio Cioffi-Revilla, Liviu Panait, Keith Sullivan, and Gabriel Balan -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/simulation.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Claudio Cioffi-Revilla, Liviu Panait, Keith Sullivan, and Gabriel Balan. 2005. MASON: a Multi-agent Simulation Environment.  <i>Simulation: Transactions of the society for Modeling and Simulation International</i>.  82(7):517-527. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h1');">Show / Hide</a><dd id='h1' style="display:none;"> We introduce MASON, a fast, easily extensible, discrete-event multi-agent simulation toolkit in Java.  MASON was designed to serve as the basis for a wide range of multi-agent simulation tasks ranging from swarm robotics to machine learning to social complexity environments.  MASON carefully delineates between model and visualization, allowing models to be dynamically detached from or attached to visualizers, and to change platforms mid-run.  We describe the MASON system, its motivation, and its basic architectural design.  We then compare MASON to related multi-agent libraries in the public domain, and discuss six applications of the system we have built over the past year to suggest its breadth of utility.

       </dl>


  <a name="MnemonicStructureAndSocialityAComputationalAgentBasedSimulationModel"></a><tr><td class="dte">Mnemonic Structure and Sociality: a Computational Agent-based Simulation Model</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Claudio Cioffi-Revilla, Sean Paus, Sean Luke, James Olds, and Jason Thomas -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/siena.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Claudio Cioffi-Revilla, Sean Paus, Sean Luke, James Olds, and Jason Thomas. 2004. Mnemonic Structure and Sociality: a Computational Agent-based Simulation Model.  In <i>Proceedings of the Conference on Collective Intentionality IV</i>.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f8');">Show / Hide</a><dd id='f8' style="display:none;">How does group memory affect sociality? Most computational multi-agent social simulation models are designed with agents lacking explicit internal information-processing structure in terms of basic cognitive elements. In particular, memory is usually not explicitly modeled. We present initial results from a new prototype called "Wetlands", designed to investigate the effect of group memory structures and interaction situations on emergent patterns of sociality or collective intentionality. Specifically, we report on initial computational experiments conducted on culturally-differentiated agents endowed with finite and degradable memory that simulate bounded mnemonic function and forgetfulness. Our main initial findings are that memory capacity and engram retention both promote sociality among groups, probably as nonlinear (inverse) functions. Wetlands 1.1 is implemented in the new MASON 3 (Multi- Agent Simulator of Networks and Neighborhoods) computational environment developed at George Mason University. 

       </dl>

       
	<a name="APheromoneBasedUtilityModelForCollaborativeForaging"></a><tr><td class="dte">A Pheromone-based Utility Model for Collaborative Foraging</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/panait04pheromone.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2004. A Pheromone-based Utility Model for Collaborative Foraging.  In <i>Proceedings of the Third International Joint Conference on Autonomous Angents and Multi Agent Systems (AAMAS)</i>, pages 36-43.

	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a5');">Show / Hide</a><dd id='a5' style="display:none;"> 
Multi-agent research often borrows from biology, where remarkable examples of collective intelligence may be found. One interesting example is ant colonies' use of pheromones as a joint communication mechanism. In this paper we propose two pheromone-based algorithms for artificial agent foraging, trail-creation, and other tasks. Whereas practically all previous work in this area has focused on biologically-plausible but ad-hoc single pheromone models, we have developed a formalism which uses multiple pheromones to guide cooperative tasks. This model bears some similarity to reinforcement learning. However, our model takes advantage of symmetries common to foraging environments which enables it to achieve much faster reward propagation than reinforcement learning does. Using this approach we demonstrate cooperative behaviors well beyond the previous ant-foraging work, including the ability to create optimal foraging paths in the presence of obstacles, to cope with dynamic environments, and to follow tours with multiple waypoints. We believe that this model may be used for more complex problems still. 
      </dl>


     <a name="AntForagingRevisited"></a><tr><td class="dte">Ant Foraging Revisited</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/panait04ant.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2004.  Ant Foraging Revisited.  In <i>Proceedings of the Ninth 
International Conference on the Simulation and Synthesis of Living Systems (ALIFE9)</i>, pages 
569-574. 

	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a6');">Show / Hide</a><dd id='a6' style="display:none;"> 
Most previous artificial ant foraging algorithms have to date relied to some degree on a priori knowledge of the environment, in the form of explicit gradients generated by the nest, by hard-coding the nest location in an easily-discoverable place, or by imbuing the artificial ants with the knowledge of the nest direction. In contrast, the work presented solves ant foraging problems using two pheromones, one applied when searching for food and the other when returning food items to the nest. This replaces the need to use complicated nest-discovery devices with simpler mechanisms based on pheromone information, which in turn reduces the ant system complexity. The resulting algorithm is orthogonal and simple, yet ants are able to establish increasingly efficient trails from the nest to the food in the presence of obstacles. The algorithm replaces the blind addition of new amounts of pheromones with an adjustment mechanism that resembles dynamic programming. 
      </dl>


      	<a name="LearningAntForagingBehaviors"></a><tr><td class="dte">Learning Ant Foraging Behaviors</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/panait04learning.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2004.  Learning ant foraging behaviours. In Jordan Pollack, et al., editors, <i>Artificial Life IX: Ninth International Conference on the Simulation and Synthesis of Living Systems</i>, pages 575-580. The MIT Press. 
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a7');">Show / Hide</a><dd id='a7' style="display:none;"> 
Insects are good at cooperatively solving many complex tasks. For example, foraging for food far away from a nest can be solved through relatively simple behaviors in combination with pheromones. As task complexity increases, however, it may become difficult to find individual agent rules which yield a desired emergent cooperative behavior, or to know if any such rules exist at all. For such tasks, machine learning techniques like evolutionary computation (EC) may prove a valuable approach to searching the space of possible rule combinations. This paper presents an application of genetic programming to search for foraging behaviors. The learned foraging behaviors use only pheromone information to find the path to the nest and to the food source. 
      </dl>


     <a name="AntForagingRevisited2"></a><tr><td class="dte">Ant Foraging Revisited</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Liviu Panait and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/AntForagingRevisited03.pdf">PDF</a>
	 <dt><i>Note: </i><dd>
This one-page poster was considerably extended and improved in a better follow-on paper by the same name: <a href="index.html#AntForagingRevisited">Ant Foraging Revisited</a>.
	 <dt><i>Citation:</i><dd>Liviu Panait and Sean Luke, 2003. Ant Foraging Revisited. In <i>Proceedings of the 
Second International Workshop on the Mathematics and Algorithms of Social Insects</i>, page 184. 
	 <dt><i>First Paragraph: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d7');">Show / Hide</a><dd id='d7' style="display:none;">
	   Previous artificial (non-biological) ant foraging models have to date relied to some degree on a priori knowledge of the environment, in the form of explicit gradients generated by the nest, by hard-coding the nest location in an easily-discoverable place, or by imbuing the artificial ants with the knowledge of the nest direction. In contrast, the work presented solves ant foraging problems using two pheromones, one applied when searching for food and the other when returning food items to the nest. This replaces the need for using complicated devices to locate the nest source with simpler mechanisms based on pheromone information, which in turn reduces the ant system complexity. The resulting algorithm is orthogonal and simple, yet ants are able to establish increasingly efficient trails from the nest to the food in the presence of obstacles.
       </dl>
       
  <a name="EvolvingForagingBehaviors"></a><tr><td class="dte">Evolving Foraging Behaviors</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Liviu Panait and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/EvolvingForagingBehaviors.pdf">PDF</a>
	 <dt><i>Note: </i><dd>
This small paper was considerably extended and improved in a better follow-on paper, <a href="index.html#LearningAntForagingBehaviors">Learning Ant Foraging Behaviors</a>.
	 <dt><i>Citation:</i><dd>Liviu Panait and Sean Luke, 2003. Evolving foraging behaviors. In <i>Proceedings of the 
Second International Workshop on the Mathematics and Algorithms of Social Insects</i>, pages 
131-138. 
	 <dt><i>First Paragraph: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d8');">Show / Hide</a><dd id='d8' style="display:none;">
Insects are particularly good at cooperatively solving multiple complex tasks.  Some such tasks, such a foraging for food far away from the nest or clustering objects into piles, can be solved through relatively simple behaviors in combination with communication mechanisms using pheromones.  As task complexity increases, however, it may become difficult to determine the proper simple rules which yield the desired emergent cooperative behavior; or to know if any such rules exist at all.  For such tasks, machine learning techniques like evolutionary computation (EC) may prove a valuable approach to searching the space of possible rule combinations.
       </dl>



              <a name="MASONAMultiagentSimulationLibrary2"></a><tr><td class="dte">MASON: A Multiagent Simulation Library</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke, Gabriel Catalin Balan, Liviu Panait, Claudio Cioffi-Revilla, and Sean Paus -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/Agent2003.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Gabriel Catalin Balan, Liviu Panait, Claudio Cioffi-Revilla, and Sean Paus. 2003. MASON: A Multiagent Simulation Library.  In <i>Proceedings of the Agent 2003 Conference on Challenges in Social Simulation</i>.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f3');">Show / Hide</a><dd id='f3' style="display:none;">Agent-based modeling (ABM) has transformed social science research by allowing researchers to replicate or generate the emergence of empirically complex social phenomena from a set of relatively simple agent-based rules at the micro-level. Swarm, RePast, Ascape, and others currently provide simulation environments for ABM social science research. After Swarm -- arguably the first widely used ABM simulator employed in the social sciences -- subsequent simulators have sought to enhance available simulation tools and computational capabilities by providing additional functionalities and formal modeling facilities. Here we present MASON (Multi-Agent Simulator Of Neighborhoods), following in a similar tradition that seeks to enhance the power and diversity of the available scientific toolkit in computational social science. MASON is intended to provide a core of facilities useful not only to social science but to other agent-based modeling fields such as artificial intelligence and robotics. We believe this can foster useful "cross-pollination" between such diverse disciplines, and further that MASON's additional facilities will become increasing important as social complexity simulation matures and grows into new approaches. We illustrate the new MASON simulation library with a replication of HeatBugs and a demonstration of MASON applied to two challenging case studies: ant-like foragers and micro-aerial agents. Other applications are also being developed. The HeatBugs replication and the two new applications provide an idea of MASON's potential for computational social science and artificial societies. 
 
       </dl>

       <a name="ReplicationOfSugarscapeUsingMASON"></a><tr><td class="dte">Replication of Sugarscape Using MASON</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Anthony Bigbee, Claudio Cioffi-Revilla, and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/Replication07.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Anthony Bigbee, Claudio Cioffi-Revilla, and Sean Luke. 2005.  Replication of Sugarscape Using MASON.  In <i>Agent-based Approaches in Economic and Social Complex systems IV: Post-Proceedings of the AESCS International Workshop</i>.  Vo. 3.  183-190.  Springer.
	 <dt><i>First Paragraph: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e1');">Show / Hide</a><dd id='e1' style="display:none;">
The purpose of this research was to replicate the Sugarscape model (Eptstein and Axtell 1996) and simulation outcomes as described in Growing Artificial Societies (GAS). Sugarscape is a classic agent-based model and contemporary simulation toolkits usually only have a very simple replication of a few core rules. There is scant evidence of significant replication of the rules and simulation outcomes; code supplied with Repast, Swarm, and NetLogo implement a minority of the rules in Sugarscape. In particular, the standard Repast distribution only implements Growback, Movement, and Replacement. Sugarscape implementations in these toolkits are clearly provided only as basic demonstrations of how wellknown social models might be implemented, rather than complete achievements of scientific replication.
       </dl>


       <a name="MASONANewMultiagentSimulationToolkit"></a><tr><td class="dte">MASON: A New Multi-agent Simulation Toolkit</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Sean Luke, Claudio Cioffi-Revilla, Liviu Panait, and Keith Sullivan -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/SwarmFest04.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Claudio Cioffi-Revilla, Liviu Panait, and Keith Sullivan. 2004.  MASON: A New Multi-agent Simulation Toolkit.  In <i>Proceedings of the 2004 SwarmFest Workshop</i>.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a2');">Show / Hide</a><dd id='a2' style="display:none;">
We introduce MASON, a fast, easily extendable, discrete-event multi-agent simulation toolkit in Java.  MASON was designed to serve as the basis for a wide range of multi-agent simulation tasks ranging from swarm robotics to machine learning to social complexity environments.  MASON carefully delineates between model and visualization, allowing models to be dynamically detached from or attached to visualizers, and to change platforms mid-run.  We describe the MASON system, its motivation, and its basic architectural design.  We then discuss five applications of MASON we have built over the past year to suggest its breadth of utility.
       </dl>


       <a name="MASONAJavaMultiagentSimulationLibrary"></a><tr><td class="dte">MASON: A Java Multi-agent Simulation Library</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Sean Luke, Gabriel Catalin Balan, and Liviu Panait -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/MASONSocialInsects.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Gabriel Catalin Balan, and Liviu Panait. 2003. MASON: A Java Multi-agent Simulation Library. In <i>Proceedings of the 
Second International Workshop on the Mathematics and Algorithms of Social Insects</i>.
	 <dt><i>First Paragraph: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d6');">Show / Hide</a><dd id='d6' style="display:none;">
	   
	  We present MASON, a new multiagent simulation library written for Java. 
MASON is a general-purpose, single-process, discrete-event simulation library in- 
tended to support diverse multiagent experiments ranging from 3D continuous robotics 
to social complexity networks to discretized ant foraging algorithms.
       </dl>




















       <tr><td colspan=2 class="spread"><a name="Multiagent"></a>Multiagent Learning and Fairness</td>
     </tr>


<a name="UnlearningfromDemonstration"></a><tr><td class="dte">Unlearning from Demonstration</td><td class="dde">
	<dl>
	<a href="https://cs.gmu.edu/~sean/papers/ijcai13unlearning.pdf">PDF</a>
	<dt><i>Citation:</i><dd>Keith Sullivan, Ahmned ElMolla, Bill Squires, and Sean Luke.  2013.  Unlearning from Demonstration.  In <i>Proceedings of the 23rd International Joint Conference on Artificial Intelligence</i> (IJCAI13).  
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('m12');">Show / Hide</a><dd id='m12' style="display:none;">
	When doing learning from demonstration, it is often the case that the demonstrator provides corrective examples to fix errant behavior by the agent or robot. We present a set of algorithms which use this corrective data to identify and remove noisy examples in datasets which caused errant classifications, and ultimately errant behavior. The objective is to actually modify the source datasets rather than solely rely on the noise-insensitivity of the classification algorithm. This is particularly useful in the sparse datasets often found in learning from demonstration experiments. Our approach tries to distinguish between noisy misclassification and mere undersampling of the learning space. If errors are a result of misclassification, we potentially remove the responsible points and update the classifier. We demonstrate our method on UCI Machine Learning datasets at different levels of sparsity and noise, using decision trees, K-Nearest-Neighbor, and support vector machines.
	</dd>

       <a name="MultiagentSupervisedTrainingWithAgentHierarchiesAndManualBehaviorDecomposition"></a><tr><td class="dte">Multiagent Supervised Training with Agent Hierarchies and Manual Behavior Decomposition</td><td class="dde">
       <dl>
        <a href="https://cs.gmu.edu/~sean/papers/ijcai11.pdf">PDF</a>
<dt><i>Citation:</i><dd>Keith Sullivan and Sean Luke.  2011. Multiagent Supervised Training with Agent Hierarchies and Manual Behavior Decomposition.  In <i>Proceedings of the IJCAI 2011 Workshop on Agents Learning Interactively from Human Teachers</i> (ALIHT).
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('z3');">Show / Hide</a><dd id='z3' style="display:none;">
       We present a supervised learning from demonstration system capable of training stateful and recurrent behaviors, both in the single agent and multiagent case.  Furthermore, behavior complexity due to statefulness and multiple agents can result in a high dimensional learning space, which can require many samples to learn properly. Our approach, which relies heavily on both per-agent behavior decomposition and structuring agents into a tree hierarchy, can significantly reduce the number of samples and make such training feasible. We demonstrate our system in a simulated collective foraging task where all the agents execute the same behavior set. We also discuss how to extend our approach to a heterogeneous case, where different subgroups of agents perform different behaviors.         
</dl>


   
       <a name="LongTermFairnessWithBoundedWorstCaseLosses"></a><tr><td class="dte">Long-term Fairness with Bounded Worst-case Losses</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Gabriel Balan and Dana Richards and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	  (Early Workshop Paper)   <a href="https://cs.gmu.edu/~sean/papers/aaaibounded2008.pdf">PDF</a>
	<br>(Technical Report, Similar to Journal Article) <a href="https://cs.gmu.edu/~sean/papers/GMU-CS-TR-2008-2.pdf">PDF</a>
<dt><i>Note:</i><dd>This paper began as a workshop paper, then was revised into a journal article.  Along the way we created a tech report which is very close to the journal article.
<dt><i>Citation (Workshop Paper):</i><dd>Gabriel Balan and Dana Richards and Sean Luke.  2008. Long-term Fairness with Bounded Worst-case Losses. In <i>Proceedings of AAAI Advances in Preference Workshop</i>. 7-12.
<dt><i>Citation (Journal Article):</i><dd>Gabriel Balan and Dana Richards and Sean Luke.  2011.  Long-term fairness with bounded worst-case losses.  <i>Autonomous Agents and Multiagent Systems</i>. 22(1) 43-63.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e2');">Show / Hide</a><dd id='e2' style="display:none;">
How does one repeatedly choose actions so as to be fairest to multiple beneficiaries of those actions?  We examine approaches to discovering sequences of actions for which the worst-off beneficiaries are treated maximally well, then secondarily the second-worst-off, and so on.  We formulate the problem for the situation where the sequence of action choices continues forever; this problem may be reduced to a set of linear programs.  We then extend the problem to situations where the game ends at some unknown finite time in the future.  We demonstrate that an optimal solution is NP-hard, and present two good approximation algorithms.
       </dl>



       	                      <a name="TheoreticalAdvantagesofLenientLearners:AnEvolutionaryGameTheoreticPerspective"></a><tr><td class="dte">Theoretical Advantages of Lenient Learners: an Evolutionary Game Theoretic Perspective</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, Karl Tuyls, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/panait08a.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, Karl Tuyls, and Sean Luke. 2008. Theoretical Advantages of Lenient Learners: an Evolutionary Game Theoretic Perspective.  <i>Journal of Machine Learning Research</i>.  9(March):423-457. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h4');">Show / Hide</a><dd id='h4' style="display:none;">This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the benefits of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insignificant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. 

       </dl>     

	<a name="AnOverviewOfCooperativeAndCompetitiveMultiagentLearning"></a><tr><td class="dte">An Overview of Cooperative and Competitive Multiagent Learning</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/LAMAS05Overview.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Pieter Jan 't Hoen, Karl Tuyls, Liviu Panait, Sean Luke, and J. A. La Poutr&eacute;.  2005. An Overview of Cooperative and Competitive Multiagent Learning.  In <i>Learning and Adaptation in Multi-Agent Systems, First International Workshop (LAMAS)</i>.  1-46.  Springer.
	  
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d0');">Show / Hide</a><dd id='d0' style="display:none;"> 
<i>Multi-agent systems</i> (MASs) is an area of distributed artificial intelligence that emphasizes the joint behaviors of agents with some 
degree of autonomy and the complexities arising from their interactions. 
The research on MASs is intensifying, as supported by a growing number of conferences, workshops, and journal papers. In this survey we give 
an overview of multi-agent learning research in a spectrum of areas, including reinforcement learning, evolutionary computation, game theory, 
complex systems, agent modeling, and robotics. 
MASs range in their description from cooperative to being competitive 
in nature. To muddle the waters, competitive systems can show apparent 
cooperative behavior, and vice versa. In practice, agents can show a wide 
range of behaviors in a system, that may either fit the label of cooperative 
or competitive, depending on the circumstances. In this survey, we discuss current work on cooperative and competitive MASs and aim to make 
the distinctions and overlap between the two approaches more explicit. 
Lastly, this paper summarizes the papers of the first International 
workshop on Learning and Adaptation in MAS (LAMAS) hosted at the 
fourth International Joint Conference on Autonomous Agents and Multi 
Agent Systems (AAMAS'05) and places the work in the above survey. 

      </dl>

 <a name="Can GoodLearnersAlwaysCompensateforPoorLearners?"></a><tr><td class="dte">Can Good Learners Always Compensate for Poor Learners?</td><td class="dde">
       <dl>
             <a href="https://cs.gmu.edu/~sean/papers/aamas06compensate.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Liviu Panait, and Sean Luke. 2006. Can Good Learners Always Compensate for Poor Learners?  In <i>Proceedings of the 2006 Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</i>.
	 <dt><i>Note:</i><dd>A fuller version of this paper may be found in the following <a href="http://cs.gmu.edu/~tr-admin/papers/GMU-CS-TR-2005-3.pdf">technical report</a> (PDF).

         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('q10');">Show / Hide</a><dd id='q10' style="display:none;">
Can a good learner compensate for a poor learner when paired in a coordination game?  Previous work has given an example where a special learning algorithm (FMQ) is capable of doing just that when paired with a specific less capable algorithm even in games which stump the poorer algorithm when paired with itself.  In this paper, we argue that this result is not general.  We give a straightforward extension to the coordination game in which FMQ cannot compensate for the lesser algorithm.  We also provide other problematic pairings, and argue that another high-quality algorithm cannot do so either. 
       </dl>


     
 <a name="LenientLearnersInCooperativeMultiagentSystems"></a><tr><td class="dte">Lenient Learners in Cooperative Multiagent Systems</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, Keith Sullivan, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/aamas06variable-short.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, Keith Sullivan, and Sean Luke. 2006. Lenient Learners in Cooperative Multiagent Systems.  In <i>Proceedings of the 2006 Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</i>. 
        <dt><i>Note:</i><dd>A fuller version of this paper may be found in the following <a href="http://cs.gmu.edu/~tr-admin/papers/GMU
-CS-TR-2013-2.pdf">technical report</a> (PDF).
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g1');">Show / Hide</a><dd id='g1' style="display:none;">In concurrent learning algorithms, an agent's perception of the joint search space depends on the actions currently chosen by the other agents. These perceptions change as each agent's action selection is influenced by its learning. We observe that agents that show lenience to their teammates achieve more accurate perceptions of the overall learning task. Additionally, lenience appears more beneficial at early stages of learning, when the agent's teammates are merely exploring their actions, and less helpful as the agents start to converge. We propose two multiagent learning algorithms where agents exhibit a variable degree of lenience, and we demonstrate their advantages in several coordination problems.
       </dl>

            
	<a name="CollaborativeMultiagentLearningTheStateOfTheArt"></a><tr><td class="dte">Collaborative Multi-agent Learning: The State of the Art</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/CMASL.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2005. Collaborative Multi-agent Learning: The State of the Art.  <i>Autonomous Agents and Multi-agent Systems</i> 11:3 (November).  387-434.

	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a3');">Show / Hide</a><dd id='a3' style="display:none;"> 
Cooperative multi-agent systems are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility.  Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication.  The challenge this presents to the task of programming solutions to multi-agent systems problems has spawned increasing interest in machine learning techniques to automate the search and optimization process.

<p>We provide a broad survey of the cooperative multi-agent learning literature.  Previous surveys of this area have largely focused on issues common to specific subareas (for example, reinforcement learning or robotics).  In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including reinforcement learning, evolutionary computation, game theory, complex systems, agent modeling, and robotics.

<p>We find that this broad view leads to a division of the work into two categories, each with its own special issues: applying a single learner to discover joint solutions to multi-agent problems (<i>team learning</i>), or using multiple simultaneous learners, often one per agent (<i>concurrent learning</i>).  Additionally, we discuss direct and indirect communication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics.  We conclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources.
      </dl>



     
	<a name="CollaborativeMultiagentLearningASurvey"></a><tr><td class="dte">Collaborative Multi-agent Learning: A Survey</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/CMASL.early.pdf">PDF</a>
	<dt><i>Note: </i><dd>
	  This is an early version of the much improved survey <a href="index.html#CollaborativeMultiagentLearningTheStateOfTheArt">Collaborative Multi-agent Learning: the State of the Art</a>, which you should read instead.
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2004. Collaborative Multi-agent Learning: A Survey.  In <i>AAAI Fall Symposium on Multiagent Learning</i>.

	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a4');">Show / Hide</a><dd id='a4' style="display:none;"> 
The field of Multiagent Systems is concerned with domains where several agents interact while solving competitive or cooperative tasks.  Increasingly complex problem domains involving non-trivial numbers of agents revealed inherent difficulties with creating such systems.  As such, the field witnessed a recent increased interest in Machine Learning techniques, capable of easing the programming efforts for approaching more challenging domains.

<P>Despite the relative youth of the field, there are already several hundreds of papers trying to answer interesting questions in domains where learning affects the behavior of more than a single agent.  Such questions target issues as varied as learning to communicate, modeling other agents in the environment, learning to compete or cooperate with the other agents, analysis of optimality for the learning algorithms, and many others.  The large variety of papers generates a desirability for good surveys categorizing previous work.

<P>We argue that there are two alternatives for learning in multiagent systems.  A first direction is to have each individual agent learn how to improve its performance.  The alternative is to have a single learning process that improves the behavior of the entire team of agents.  Because of scalability problems with respect to increased numbers of agents, the majority of machine learning techniques can not be realistically applied to learn team behaviors.  We discovered that most previous surveys concentrate on individual agents' learning, but neglect approaches at the team level.

<P>In this survey, we propose a new arrangement for multiagent learning papers.  As such, there is an entire category of papers dealing with learning behaviors for the entire team and issues associated with scalability to non-trivial numbers of agents, such as the heterogeneity of the team.  A second class of papers is concerned with individual agent learning in multiagent domains.  Based on their main research focus, the papers are categorized in sections dealing with optimality of learned behaviors, impact of locality of reward information, cooperation or competition relations among agents, and modeling other agents.

<P>Additionally, we identified two issues relatively perpendicular to the team or individual levels of learning: problem decomposition and communication.  The former is concerned with techniques for decomposing either the problem to be solved or the collective behavior into simpler independent components that can be more easily handled by the learning process.  An in-depth survey of the literature on the relationship between multiagent learning and communication revealed three approaches, each with its own particularities.  The three such methods involve communication via either rapidly decaying information, slowly decaying information, or embodiment.
      </dl>


 
















       <tr><td colspan=2 class="spread"><a name="EvolutionaryAlgorithms"></a>Evolutionary Algorithms</td>
	  </tr>

<tr><td class="dte">
	<a name="IsTheMetaEAAViableOptimizationMethod"></a>Is the Meta-EA a Viable Optimization Method?</td><td class="dde">
	<dl>
              <a href="https://cs.gmu.edu/~sean/papers/gecco13metaga.pdf">PDF</a>
	<dt><i>Citation:</i><dd>Sean Luke and AKM Khaled Ahsan Talukder.  2013.  Is the Meta-EA a Viable Optimization Method?  In <i>Proceedings
 of the 15th Annual Conference on Genetic and Evolutionary Computation
(GECCO 2013)</i>.
	<dt><i>Note:</i> 
<dd>This version has a few very minor corrections made to the original conference publication.
 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('y5');">Show / Hide</a><dd id='y5' style="display:none;">
Meta-evolutionary algorithms have long been proposed as an approach to automatically discover good parameter settings to use in later optimization runs.  In this paper we instead ask whether a meta-evolutionary algorithm makes sense as an optimizer in its own right.  That is, we're not interested in the resulting parameter settings, but only in the final result.  As it so happens, this use of meta-EAs make sense in the context of large numbers of parallel runs, particularly in massive distributed scenarios. A primary issue facing meta-EAs is the stochastic nature of the meta-level fitness function.  We consider whether this poses a challenge to establishing a gradient in the meta-level search space, and to what degree multiple tests are helpful in smoothing the noise.  We discuss the nature of the meta-level search space and its impact on local optima, then examine the degree to which exploitation can be applied.  We find that meta-EAs perform well as optimizers, and very surprisingly that they do best with only a single test.  More exploitation appears to reduce  performance, but only slightly.
</dl>

<a name="MultiobjectiveOptimizationOfCoClusteringEnsembles"></a><tr><td class="dte">Multiobjective Optimization of Co-Clustering Ensembles</td><td class="dde">
	<dl>
	<a href="https://cs.gmu.edu/~sean/papers/gecco12clustering.pdf">PDF</a>
	<dt><i>Citation:</i><dd>Francesco Gullo, AKM Khaled Ahsan Talukder, Sean Luke, Carlotta Domeniconi, and Andrea Tagarelli.  Multiobjective Optimization of Co-Clustering Ensembles.  In <i>GECCO '12: Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation</i>.  Pages 1495-1496.  ACM.
	<dt><i>Note:</i><dd>A fuller version of this paper may be found in the following <a href="http://cs.gmu.edu/~tr-admin/papers/GMU-CS-TR-2012-3.pdf">technical report</a> (PDF).
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('m13');">Show / Hide</a><dd id='m13' style="display:none;">
	Co-clustering is a machine learning task where the goal is to simultaneously develop clusters of the data and of their respective features. We address the use of co-clustering ensembles to establish a consensus co-clustering over the data. In this paper we develop a new preference-based multiobjective optimization algorithm to compete with a previous gradient ascent approach in finding optimal co-clustering ensembles. Unlike the gradient ascent algorithm, our approach once tackles the co-clustering problem with multiple heuristics, then applies the gradient ascent algorithm's joint heuristic as a preference selection procedure. We are able to significantly outperform the gradient ascent algorithm on feature clustering and on problems with smaller datasets.
</dl>
	
   <a name="FindingInterestingThingsPopulationBasedAdaptiveParameterSweeping"></a><tr><td class="dte">Finding Interesting Things: Population-based Adaptive Parameter Sweeping</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and Deepankar Sharma and Gabriel Catalin Balan  -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gecco07interesting.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke and Deepankar Sharma and Gabriel Catalin Balan. 2007. Finding Interesting Things: Population-based Adaptive Parameter Sweeping.  In <i>GECCO '07: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation</i>.  Pages 86-93.  ACM. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g7');">Show / Hide</a><dd id='g7' style="display:none;">Model- and simulation-designers are often interested not in the optimum output of their system, but in understanding how the output is sensitive to different parameters. This can require an inefficient sweep of a multidimensional parameter space, with many samples tested in regions of the space where the output is essentially all the same, or a sparse sweep which misses crucial "interesting" regions where the output is strongly sensitive. In this paper we introduce a novel population-oriented approach to adaptive parameter sweeping which focuses its samples on these sensitive areas. The method is easy to implement and model-free, and does not require any previous knowledge about the space. In a weakened form the method can operate in non-metric spaces such as the space of genetic program trees. We demonstrate the method on three test problems, showing that it identifies regions of the space where the slope of the output is highest, and concentrates samples on those regions. 

       </dl>


 <a name="OpportunisticEvolutionEfficientEvolutionaryComputationonLargeScaleComputationalGrids"></a><tr><td class="dte">Opportunistic Evolution: Efficient Evolutionary Computation on Large-Scale Computational Grids</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Keith Sullivan, Sean Luke, Curt Larock, Sean Cier, and Steven Armentrout -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gecco08opportunistic.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Keith Sullivan, Sean Luke, Curt Larock, Sean Cier, and Steven Armentrout.  2008.  Opportunistic Evolution: Efficient Evolutionary Computation on Large-Scale Computational Grids.  In <i>Genetic and Evolutionary Computation Conference Late Breaking Papers</i>. 
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e4');">Show / Hide</a><dd id='e4' style="display:none;">
We examine opportunistic evolution, a variation of master-slave distributed evaluation designed for deployment of evolutionary computation to very large grid computing architectures with limited communications, severe evaluation overhead, and wide variance in evaluation node speed.  In opportunistic evolution, slaves receive some <i>N</i> individuals at a time, evaluate them, and then run those individuals through their own mini evolutionary loop until some fixed wall clock time has been exceeded.  Our implementation of opportunistic evolution may be used in conjunction with either a generational or, for maximum throughput, an asynchronous steady-state evolutionary model in the master.  Opportunistic evolution is strongly exploitative.  We perform initial experiments comparing the technique with a traditional master/slave model, and suggest possible classes of problems for which it might be apropos.
       </dl>

<a name="AnApplicationOfEvolutionaryAlgorithmsToStudyTheExtentOfSLHFAnomalyAssociatedWithCoastalEarthquakes"></a><tr><td class="dte">An Application of Evolutionary Algorithms to Study the Extent of SLHF Anomaly Associated with Coastal Earthquakes</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Guido Cervone, Liviu Panait, Ramesh Singh, and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/cervone04application.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Guido Cervone, Liviu Panait, Ramesh Singh, and Sean Luke.  2004.  An application of evolutionary algorithms to study the extent of SLHF anomaly associated with coastal earthquakes. In <i>Late Breaking Papers of the Genetic and Evolutionary Computation Conference</i> (GECCO). Springer. 
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d6a');">Show / Hide</a><dd id='d6a' style="display:none;">
Multi sensor remote sensing provides real time high resolution data that can be used to study anomalous changes on land, in the 
ocean, and in the atmosphere associated with an impending earthquake. 
Anomalous behaviour in Surface Latent Heat Flux (SLHF) prior to large 
coastal earthquakes has been recently found. However, an SLHF time series usually contains several sharp peaks that may be associated either 
with earthquakes or with atmospheric perturbations. In this paper we 
have used evolutionary algorithms to perform a search in a large space 
bounded by longitude, latitude and time, to distinguish between signals associated with earthquakes and those associated with atmospheric 
phenomena. The algorithm finds paths which delimit the extent of the 
detected anomalies by optimizing an ob jective function that takes into 
consideration several aspects, such as spatial and time continuity, the 
magnitude of the anomalies, and the distance to the continental boundary. This search strategy is crucial for the development of a fully automated early warning system for providing information about impending 
earthquakes in a seismically active coastal region. 

<p>Experiments have been performed over a 2000 km^2 
area comprising a 
part of the continental boundary between the African and Eurasian plate, 
roughly corresponding to Italy and Greece, one of the most seismically 
active regions. Using a 365-days-long time series, we identified three signals associated with seismic events. Additionally, it was possible to establish that the extent of the signal does not propagate further than 600 
km from the epicenter of the earthquake. 
       </dl>

       







	  


       <tr><td colspan=2 class="spread"><a name="Coevolution"></a>Coevolution</td>
	  </tr>


	<a name="LargeScaleEmpiricalAnalysisOfCooperativeCoevolution"></a><tr><td class="dte">Large Scale Empirical Analysis of Cooperative Coevolution</td><td class="dde">
	<dl>
		<a href="https://cs.gmu.edu/~sean/papers/gecco11-coevolution.pdf">PDF</a> 
	<dt><i>Citation:</i><dd>Sean Luke, Keith Sullivan, and Faisal Abidi.  2011.  Large Scale Empirical Analysis of Cooperative Coevolution.  In <i>Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation (GECCO 2011). </i> Pages 151-152.  ACM.
	<dt><i>Note: </i><dd>A much fuller version of this paper may be found in the following <a href="http://cs.gmu.edu/~tr-admin/papers/GMU-CS-TR-2011-2.pdf">technical report</a> (PDF).
	<dt><i>Note: </i><dd>This is part 1 of a two paper sequence.  The second is <a href="index.html#DoMultipleTrialsHelpUnivariateMethods">Do Multiple Trials Help Univariate Methods?</a>
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('z1');">Show / Hide</a><dd id='z1' style="display:none;">
We present a study of cooperative coevolution applied to moderately complex optimization problems in large-population environments.  The study asks three questions.  First: what collaboration methods perform best, and when?  Second: how many subpopulations are desirable?  Third: is it worthwhile to do more than one trial per fitness evaluation?  We discovered that  parallel methods tended to work better than sequential ones, that &ldquo;shuffling&rdquo; (a collaboration method) predominated in performance in more complex problems, that more subpopulations generally did better, and that more trials performed marginally better.
	</dl>


               <a name="DoMultipleTrialsHelpUnivariateMethods"></a><tr><td class="dte">Do Multiple Trials Help Univariate Methods?</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/cec11-pbil.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Daniel Rothman, Sean Luke, and Keith Sullivan.  2011.  Do Multiple Trials Help Univariate Methods? 
In <i>Proceedings of the 2011 Congress of Evolutionary Computation, 2011</i>.  Pages 2391-2398.  IEEE.
		<dt><i>Note:</i><dd>This is part 2 of a two paper sequence.  The second is <a href="index.html#LargeScaleEmpiricalAnalysisOfCooperativeCoevolution">Large Scale Empirical Analysis of Cooperative Coevolution</a>
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('z0');">Show / Hide</a><dd id='z0' style="display:none;">
Cooperative Coevolutionary Algorithms (CCEAs) and Univariate Estimation of Distribution Algorithms (Univariate EDAs) are closely related algorithms in that both update marginal distributions/populations, and test samples of those distributions/populations by grouping them with collaborators drawn from elsewhere to form a complete solution.  Thus the quality of these samples is context-sensitive and the algorithms assume low linkage among their variables.  This results in well-known difficulties with these methods.  While EDAs have commonly overcome these difficulties by examining multivariate linkage, CCEAs have instead examined basing the fitness of each marginal sample on the maximum of several trials.  In this study we examine whether multiple-trial CCEA approach is really effective for difficult problems and large numbers of subpopulations; and whether this approach can be used to improve Univariate EDAs as well.
	</dl>
 
               <a name="CooperativeCoevolutionAndUnivariateEstimationOfDistributionAlgorithms"></a><tr><td class="dte">Cooperative Coevolution and Univariate Estimation of Distribution Algorithms</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Christopher Vo, Liviu Panait, and Sean Luke  -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/foga09-eda.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Christopher Vo, Liviu Panait, and Sean Luke. 2007. Cooperative Coevolution and Univariate Estimation of Distribution Algorithms.  In <i>Proceedings of the 10th ACM SIGEVO Conference on Foundations of Genetic Algorithms (FOGA)</i>.  Pages 141-150.  ACM. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g8');">Show / Hide</a><dd id='g8' style="display:none;">In this paper, we discuss a curious relationship between Cooperative Coevolutionary Algorithms (CCEAs) and univariate Estimation of Distribution Algorithms (EDAs). Specifically, the distribution model for univariate EDAs is equivalent to the infinite population EGT model common in the analysis of CCEAs. This relationship may permit cross- pollination between these two disparate fields. As an example, we derive a new EDA based on a known CCEA from the literature, and provide some preliminary experimental analysis of the algorithm.  
 
       </dl>

   <a name="BiasingCoevolutionarySearchForOptimalMultiagentBehaviors"></a><tr><td class="dte">Biasing Coevolutionary Search for Optimal Multiagent Behaviors</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, Sean Luke, and R. Paul Wiegand -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/ieeetec04-bias.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, Sean Luke, and R. Paul Wiegand. 2006. Biasing Coevolutionary Search for Optimal Multiagent Behaviors.  <i>IEEE Transactions on Evolutionary Computation</i>.  10(6):629-645. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h3');">Show / Hide</a><dd id='h3' style="display:none;"> Cooperative coevolutionary algorithms offer great potential for concurrent multiagent learning domains and are of special utility to domains involving teams of multiple agents. Unfortunately, they also exhibit pathologies resulting from their game-theoretic nature, and these pathologies interfere with finding solutions that correspond to optimal collaborations of interacting agents. We address this problem by biasing a cooperative coevolutionary algorithm in such a way that the fitness of an individual is based partly on the result of interactions with other individuals (as is usual), and partly on an estimate of the best possible reward for that individual if partnered with its optimal collaborator. We justify this idea using existing theoretical models of a relevant subclass of coevolutionary algorithms, demonstrate how to apply biasing in a way that is robust with respect to parameterization, and provide some experimental evidence to validate the biasing approach. We show that it is possible to bias coevolutionary methods to better search for optimal multiagent behaviors. 
       </dl>

       <a name="ArchiveBasedCooperativeCoevolutionaryAlgorithms"></a><tr><td class="dte">Archive-based Cooperative Coevolutionary Algorithms</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, Sean Luke, and Joseph F. Harrison -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gecco06occea.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, Keith Sullivan, and Sean Luke. 2006. Archive-based Cooperative Coevolutionary Algorithms.  In <i>Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)</i>. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g2');">Show / Hide</a><dd id='g2' style="display:none;">Archive-based cooperative coevolutionary algorithms attempt to retain a set of individuals which act as good collaborators for other coevolved individuals in the evolutionary system. We introduce a new archive-based algorithm, called iCCEA, which compares favorably with other cooperative coevolutionary algorithms. We explain the current problems with cooperative coevolution which have given rise to archive methods, detail the iCCEA algorithm, compare it against other traditional and archive-based methods on basic problem domains, and discuss the reasons behind the performance of various algorithms. 
       </dl>



       <a name="SelectingInformativeActionsImprovesCooperativeMultiagentLearning"></a><tr><td class="dte">Selecting Informative Actions Improves Cooperative Multiagent Learning</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/aamas06occea.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait and Sean Luke. 2006. Selecting Informative Actions Improves Cooperative Multiagent Learning.  In <i>Proceedings of the 2006 Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</i>. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g3');">Show / Hide</a><dd id='g3' style="display:none;">In concurrent cooperative multiagent learning, each agent simultaneously learns to improve the overall performance of the team, with no direct control over the actions chosen by its teammates. An agent's action selection directly influences the rewards received by all the agents, resulting in a co-adaptation among the concurrent learning processes. Co-adaptation can drive the team towards suboptimal solutions because agents tend to select those actions that are rewarded better, without any consideration for how such actions may affect the search of their teammates. We argue that to counter this tendency, agents should also prefer actions that inform their teammates about the structure of the joint search space in order to help them choose from among various action options. We analyze this approach in a cooperative coevolutionary framework, and we propose a new algorithm, iCCEA, that highlights the advantages of selecting informative actions. We show that iCCEA generally outperforms other cooperative coevolution algorithms on our test problems. 
       </dl>


   	<a name="TimeDependentCollaborationSchemesForCooperativeCoevolutionaryAlgorithms"></a><tr><td class="dte">Time-dependent Collaboration Schemes for Cooperative Coevolutionary Algorithms</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/panait05time.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2005.  Time-dependent Collaboration Schemes for Cooperative Coevolutionary Algorithms.  In <i> AAAI 2005 Fall Symposium on Coevolutionary and Coadaptive Systems</i>.  18-25.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d9');">Show / Hide</a><dd id='d9' style="display:none;"> 
Cooperative coevolutionary algorithms represent a popular approach to learning via problem decomposition.  Since they were proposed more than a decade ago, their properties have been studied both formally and empirically.  One important aspect of cooperative coevolutionary algorithms concerns how to select collaborators for computing the fitness of individuals in different populations.  We argue that using a fixed number of collaborators during the entire search may be suboptimal.  We experiment with a simple ad-hoc collaboration scheme that varies the numbers of collaborators over time.  Empirical comparisons in a series of problem domains indicate that decreasing the numbers of collaborators over time fares better than fixed collaboration schemes.  We conclude with a bried discussion of our findings and suggest directions for future research.
      </dl>




  <a name="AVisualDemonstrationOfConvergencePropertiesOfCooperativeCoevolution"></a><tr><td class="dte">A Visual Demonstration of Convergence Properties of Cooperative Coevolution</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, R. Paul Wiegand, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/ppsn04basins.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, R. Paul Wiegand, and Sean Luke. 2004. A Visual Demonstration of Convergence Properties of Cooperative Coevolution.  In <i>Parallel Problem Solving from Nature (PPSN 2004)</i>.  Springer.  Pages 892-901.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f4');">Show / Hide</a><dd id='f4' style="display:none;">We introduce a model for cooperative coevolutionary algorithms (CCEAs) using partial mixing, which allows us to compute the expected long-run convergence of such algorithms when individuals' fitness is based on the maximum payoff of some <i>N</i> evaluations with partners chosen at random from the other population.  Using this model, we devise novel visualization mechanisms to attempt to qualitatively explain a difficult-to-conceptualize pathology in CCEAs: the tendency for them to converge to suboptimal Nash equilibria.  We further demonstrate visually how increasing the size of <i>N</i>, or biasing the fitness to include an ideal-collaboration factor, both improve the likelihood of optimal convergence, and under which initial population configurations they are not much help.

       </dl>


           <a name="ASensitivityAnalysisOfACooperativeCoevolutionaryAlgorithmBiasedForOptimization"></a><tr><td class="dte">A Sensitivity Analysis of a Cooperative Coevolutionary Algorithm Biased for Optimization</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, R. Paul Wiegand, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gecco04-bias.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, R. Paul Wiegand, and Sean Luke. 2004. A Sensitivity Analysis of a Cooperative Coevolutionary Algorithm Biased for Optimization.  In <i>Genetic and Evolutionary Computation Conference (GECCO)</i>.  Springer.  Pages 587-584.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f5');">Show / Hide</a><dd id='f5' style="display:none;">Recent theoretical work helped explain certain optimization-related pathologies in cooperative coevolutionary algorithms (CCEAs). Such explanations have led to adopting specific and constructive strategies for improving CCEA optimization performance by biasing the algorithm toward ideal collaboration. This paper investigates how sensitivity to the degree of bias (set in advance) is affected by certain algorithmic and problem properties. We discover that the previous static biasing approach is quite sensitive to a number of problem properties, and we propose a stochastic alternative which alleviates this problem. We believe that finding appropriate biasing rates is more feasible with this new biasing technique. 
       </dl>




       <a name="MethodsForEvolvingRobustPrograms"></a><tr><td class="dte">Methods for Evolving Robust Programs</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gecco03-robust.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait and Sean Luke. 2003. Methods for Evolving Robust Programs.  <i>In Genetic and Evolutionary Computation (GECCO-2003)</i>.  Springer.  Pages 1740-1751.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f1');">Show / Hide</a><dd id='f1' style="display:none;">Many evolutionary computation search spaces require fitness assessment through the sampling of and generalization over a large set of possible cases as input. Such spaces seem particularly apropos to Genetic Programming, which notionally searches for computer algorithms and functions. Most existing research in this area uses ad-hoc approaches to the sampling task, guided more by intuition than understanding. In this initial investigation, we compare six approaches to sampling large training case sets in the context of genetic programming representations. These approaches include fixed and random samples, and adaptive methods such as coevolution or fitness sharing. Our results suggest that certain domain features may lead to the preference of one approach to generalization over others. In particular, coevolution methods are strongly domain-dependent. We conclude the paper with suggestions for further investigations to shed more light onto how one might adjust fitness assessment to make various methods more effective. 
 
       </dl>

       
  <a name="ImprovingCoevolutionarySearchForOptimalMultiagentBehaviors"></a><tr><td class="dte"> Improving Coevolutionary Search for Optimal Multiagent Behaviors</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Liviu Panait, R. Paul Wiegand, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/ijcai03-bias.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait, R. Paul Wiegand, and Sean Luke. 2003. Improving Coevolutionary Search for Optimal Multiagent Behaviors. In <i>Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03)</i>.   Georg Gottlob and Toby Walsh, editors.  Pages 653-658. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f0');">Show / Hide</a><dd id='f0' style="display:none;">Evolutionary computation is a useful technique for learning behaviors in multiagent systems. Among the several types of evolutionary computation, one natural and popular method is to coevolve multiagent behaviors in multiple, cooperating populations. Recent research has suggested that coevolutionary systems may favor stability rather than performance in some domains. In order to improve upon existing methods, this paper examines the idea of modifying traditional coevolution, biasing it to search for maximal rewards. We introduce a theoretical justification of the improved method and present experiments in three problem domains. We conclude that biasing can help coevolution find better results in some multiagent problem domains. 
       </dl>

       
 
                      <a name="GuaranteeingCoevolutionaryObjectiveMeasures"></a><tr><td class="dte">Guaranteeing Coevolutionary Objective Measures</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and R. Paul Wiegand  -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/foga02-msr.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke and R. Paul Wiegand. 2002. Guaranteeing Coevolutionary Objective Measures.  In <i>Foundations of Genetic Algorithms VII</i>.  Pages 237-251.  Morgan Kaufman. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g9');">Show / Hide</a><dd id='g9' style="display:none;">The task of understanding the dynamics of coevolutionary algorithms or com- paring performance between such algorithms is complicated by the fact the internal tness measures are subjective. Though several techniques have been proposed to use external or objective measures to help in analysis, there are clearly properties of fitness payoff , like intransitivity, for which these techniques are ineffective. We feel that a principled approach to this problem is to rst establish the theoretical bounds to guarantee objective measures in one CEA model; from there one can later examine the e ects of deviating from the assumptions made by these bounds. To this end, we present a model of compet- itive tness assessment with a single population and non-parametric selection (such as tournament selection), and show minimum conditions and examples under which an objective measure exists, and when the dynamics of the coevolutionary algorithm are identical to those of a traditional EA. 
 
 
       </dl>

     <a name="AComparisonOfTwoCompetitiveFitnessFunctions"></a><tr><td class="dte">A Comparison of Two Competitive Fitness Functions</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Liviu Panait and Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/twocompetitive.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Liviu Panait and Sean Luke.  2002.  A Comparison of Two Competitive Fitness Functions.  In <i>GECCO-2002: Proceedings of the Genetic and Evolutionary Computation Conference</i>.  W. B. Langdon <i>et al</i>, eds.  Morgan Kauffman.  503-511.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a8');">Show / Hide</a><dd id='a8' style="display:none;"> 
Competitive fitness is the assessment of an individual's fitness in the context of competition with other individuals in the evolutionary system.  This commonly takes one of two forms: one-population competitive fitness, where competition is solely between individuals in the same population; and N-population competitive fitness, often termed competitive coevolution.  In this paper we discuss common topologies for one-population competitive fitness functions, then test the performance of two such topologies, Single-Elimination Tournament and K-Random Opponents, on four problem domains.  We show that neither of the extremes of K-Random Opponents (Round Robin and Random-Pairing) gives the best results when using limited computational resources.  We also show that while Single-Elimination Tournament usually outperforms variations of K-Random Opponents in noise-free problems, it can suffer from premature convergence in noisy domains.
	</dl>
      

	<a name="WhenCoevolutionaryAlgorithmsExhibitEvolutionaryDynamics"></a><tr><td class="dte">When Coevolutionary Algorithms Exhibit Evolutionary Dynamics</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Sean Luke and R. Paul Wiegand -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/gecco02-msr.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Sean Luke and R. Paul Wiegand.  2002.  When Coevolutionary Algorithms Exhibit Evolutionary Dynamics.  In the <i>Workshop on Understanding Coevolution: Theory and Analysis of Coevolutionary Algorithms (at GECCO 2002)</i>.  A. Barry, ed.  236-241.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a9');">Show / Hide</a><dd id='a9' style="display:none;"> 
The task of understanding the dynamics of coevolutionary algorithms or comparing
performance between such algorithms is complicated by the fact the internal fitness
measures are subjective.  Though a variety of techniques have been proposed to
use external or objective measures to help in analysis, there are clearly properties
of fitness payoff (e.g., intransitivity) which call such methods into question in certain
contexts.  We present a model of competitive fitness assessment with a single 
population and non-parametric selection (such as tournament selection), and show
minimum conditions and examples under which an objective measure exists,
and when the dynamics of the coevolutionary algorithm are identical to those of
a traditional EA.  
We also discuss terminological difficulties in the coevolution
literature, and present a detailed description of external measures presently
in use in the literature.
	</dl>
      











       


<tr><td colspan=2 class="spread"><a name="GeneticProgramming"></a>Genetic Programming</td>
	  </tr>

        <a name="GeneticProgrammingNeedsBetterBenchmarks"></a><tr><td class="dte">Genetic Programming Needs Better Benchmarks</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/gecco12benchmarks3.pdf">PDF</a>
	<dt><i>Note:</i><dd>This is the third version of the paper, and differs from the original publication slightly, by fixing a few errors in certain benchmark specifications.
	<dt><i>Note:</i><dd>This paper was produced in concert with <a href="http://gpbenchmarks.org">gpbenchmarks.org</a>
	<dt><i>Note:</i><dd>The symbolic regression benchmarks in this paper may be found in <a href="http://cs.gmu.edu/~eclab/projects/ecj/">ECJ</a>.
         <dt><i>Citation:</i><dd>James McDermott and David R. White and Sean Luke and Luca Manzoni and Mauro Castelli and Leonardo Vanneschi and Wojciech Ja&#347;kowski and Krzysztof Krawiec and Robin Harper and Kenneth De Jong and Una-May O'Reilly.  2012.  Genetic Programming Needs Better Benchmarks.  In <i>GECCO '12: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation</i>.  ACM.

         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h6');">Show
/ Hide</a><dd id='h6' style="display:none;">Genetic programming (GP) is not a field noted for the rigor of its benchmarking. Some of its benchmark problems are popular purely through historical contingency, and they can be criticized as too easy or as providing misleading information concerning real-world performance, but they persist largely because of inertia and the lack of good alternatives. Even where the problems themselves are impeccable, comparisons between studies are made more difficult by the lack of standardization. We argue that the definition of standard benchmarks is an essential step in the maturation of the field. We make several contributions towards this goal. We motivate the development of a benchmark suite and define its goals; we survey existing practice; we enumerate many candidate benchmarks; we report progress on reference implementations; and we set out a concrete plan for gathering feedback from the GP community that would, if adopted, lead to a standard set of benchmarks.
       </dl>


	

        <a name="EvolvingKernelsForSupportVectorMachineClassification"></a><tr><td class="dte">Evolving Kernels for Support Vector Machine Classification</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Keith M. Sullivan and Sean Luke  -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gecco07kernel.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Keith M. Sullivan and Sean Luke. 2007. Evolving Kernels for Support Vector Machine Classification.  In <i>GECCO '07: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation</i>.  Pages 1702-1707.  ACM. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('g6');">Show / Hide</a><dd id='g6' style="display:none;">While support vector machines (SVMs) have shown great promise in supervised classification problems, researchers have had to rely on expert domain knowledge when choosing the SVM's kernel function. This project seeks to replace this expert with a genetic programming (GP) system. Using strongly typed genetic programming and principled kernel closure properties, we introduce a new algorithm, called KGP, which finds near-optimal kernels. The algorithm shows wide applicability, but the combined computational overhead of GP and SVMs remains a major unresolved issue. 
       </dl>



    <a name="ADemonstrationOfNeuralProgrammingAppliedToNonMarkovianProblems"></a><tr><td class="dte">A Demonstration of Neural Programming Applied to Non-Markovian Problems</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Gabriel Catalin Balan and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/np-state.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Gabriel Catalin Balan and Sean Luke. 2004. A Demonstration of Neural Programming Applied to Non-Markovian Problems.  In <i>Genetic and Evolutionary Computation Conference (GECCO)</i>.  Springer.  Pages 422-433.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f7');">Show / Hide</a><dd id='f7' style="display:none;">Genetic programming may be seen as a recent incarnation of a long-held goal in evolutionary computation: to develop actual computational devices through evolutionary search. Genetic programming is particularly attractive because of the generality of its application, but it has rarely been used in environments requiring iteration, recursion, or internal state. In this paper we investigate a version of genetic programming developed originally by Astro Teller called neural programming. Neural programming has a cyclic graph representation which lends itself naturally to implicit internal state and recurrence, but previously has been used primarily for problems which do not need these features. In this paper we show a successful application of neural programming to various partially observable Markov decision processes, originally developed for the learning classifier system community, and which require the use of internal state and iteration. 
       </dl>

       
                <a name="PopulationImplosionInGeneticProgramming"></a><tr><td class="dte">Population Implosion in Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke, Gabriel Balan, and Liviu Panait -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/changepop.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Gabriel Catalin Balan, and Liviu Panait. 2003. Population Implosion in Genetic Programming.  In <i>Genetic and Evolutionary Computation (GECCO-2003)</i>.  Springer.  Pages 1729-1739.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f2');">Show / Hide</a><dd id='f2' style="display:none;">With the exception of a small body of adaptive-parameter literature, evolutionary computation has traditionally favored keeping the population size constant through the course of the run. Unfortunately, genetic programming has an aging problem: for various reasons, late in the run the technique become less effective at optimization. Given a fixed number of evaluations, allocating many of them late in the run may thus not be a good strategy. In this paper we experiment with gradually decreasing the population size throughout a genetic programming run, in order to reallocate more evaluations to early generations. Our results show that over four problem domains and three different numbers of evaluations, decreasing the population size is always as good as, and frequently better than, various fixed-sized population strategies. 
       </dl>

       	  	<a name="IsThePerfectTheEnemyOfTheGood"></a><tr><td class="dte">Is the Perfect the Enemy of the Good?</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Sean Luke and Liviu Panait -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/ideal.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Sean Luke and Liviu Panait.  2002.  Is the Perfect the Enemy of the Good?  In <i>GECCO-2002: Proceedings of the Genetic and Evolutionary Computation Conference</i>.  W. B. Langdon <i>et al</i>, eds.  Morgan Kauffman.  820-828.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a0');">Show / Hide</a><dd id='a0' style="display:none;"> 
Much of the genetic programming literature compares techniques using counts of ideal solutions found.  These counts in turn form common comparison measures such as Koza's Computational Effort or cumulative Probability of Success.  The use of these measures continues despite past warnings that they are not statistically valid.  In this paper we too criticize the measures for serious statistical problems, and also argue that their motivational justification is faulty.  We then present evidence suggesting that ideal solution counts are not necessarily positively related to best-fitness-of-run statistics: in fact they are often inversely correlated.  Thus claims based on ideal solution counts can mislead readers into thinking techniques will provide superior final results, when in fact the opposite is true.
      </dl>
      								
	  	<a name="WhenShortRunsBeatLongRuns"></a><tr><td class="dte">When Short Runs Beat Long Runs</td><td class="dde">
	     <dl>
	<!--<dt><i>Author: </i><dd>Sean Luke -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/geccorestarts.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Sean Luke.  2001.  When short runs beat long runs.   In <i>GECCO-2001: Proceedings of the Genetic and Evolutionary Computation Conference</i>. Lee Spector <i> et al</i>, eds.  Morgan Kaufmann.  74-80.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b1');">Show / Hide</a><dd id='b1' style="display:none;"> 
	     What will yield the best results:  doing one run <i>n</i> generations long or doing <i>m</i> runs <i>n/m</i> generations long each?  This paper presents a technique-independent analysis which answers this question, and has direct applicability to scheduling and restart theory in evolutionary computation and other stochastic methods.  The paper then applies this technique to three problem domains in genetic programming.  It discovers that in two of these domains there is a maximal number of generations beyond which it is irrational to plan a run; instead it makes more sense to do multiple shorter runs.
      </dl>


      <a name="ASurveyAndComparisonOfTreeGenerationAlgorithms"></a><tr><td class="dte">A Survey and Comparison of Tree Generation Algorithms</td><td class="dde">
      <dl>
	<!--<dt><i>Authors: </i><dd>Sean Luke and Liviu Panait -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/treegenalgs.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Sean Luke and Liviu Panait.  2001.  A survey and comparison of tree generation algorithms.  In <i>GECCO-2001: Proceedings of the Genetic and Evolutionary Computation Conference</i>. Lee Spector <i> et al</i>, eds.  Morgan Kaufmann.  81-88.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b2');">Show / Hide</a><dd id='b2' style="display:none;">
	     This paper discusses and compares five major tree-generation algorithms for genetic programming, and their effects on fitness:  <tt>RAMPED HALF-AND-HALF, PTC1, PTC2, RANDOM-BRANCH, </tt>and <tt>UNIFORM</tt>.  The paper compares the performance of these algorithms on three genetic programming problems (11-Boolean Multiplexer, Artificial Ant, and Symbolic Regression), and discovers that the algorithms do not have a significant impact on fitness.  Additional experimentation shows that tree size does have an important impact on fitness, and further that the ideal initial tree size is very different from that used in traditional GP.
      </dl>
           
	
   <a name="TwoFastTreeCreationAlgorithmsforGeneticProgramming"></a><tr><td class="dte">Two Fast Tree-Creation Algorithms for Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Author: </i><dd>Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	 <dd><a href="https://cs.gmu.edu/~sean/papers/treecreation.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Sean Luke. 2000. Two fast tree-creation algorithms for genetic programming.  In <i>IEEE Transactions on Evolutionary Computation</i> 4:3 (September 2000), 274-283. IEEE.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b3');">Show / Hide</a><dd id='b3' style="display:none;">
	   Genetic programming is an evolutionary optimization method that produces functional programs to solve a given task.  These programs commonly take the form of trees representing LISP s-expressions, and a typical evolutionary run produces a great many of these trees.  For this reason, a good tree-generation algorithm is very important to genetic programming.  This paper presents two new tree-generation algorithms for genetic programming and for "strongly-typed" genetic programming, a common variant.  These algorithms are fast, allow the user to request specific tree sizes, and guarantee probabilities of certain nodes appearing in trees.  The paper analyzes these two algorithms and compares them with traditional and recently proposed approaches.
       </dl>
       

<a name="GeneticProgramming"></a><tr><td class="dte">&ldquo;Genetic&rdquo; Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke, Shugo Hamahashi, and Hiroaki Kitano -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/gene-gecco99.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Sean Luke, Shugo Hamahashi, and Hiroaki Kitano.  1999.  "Genetic" programming.
	      In <i>GECCO-99: Proceedings of the Genetic and Evolutionary
	          Computation Conference</i>, Banzhaf, W. <i>et al</i>, eds.
	          San Fransisco: Morgan Kaufmann.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b4');">Show / Hide</a><dd id='b4' style="display:none;">
	      Much of evolutionary computation was inspired by Mendelian genetics.
	      But modern genetics has since advanced considerably, revealing that
	      genes are not simply parameter settings, but interactive cogs in a
	      complex chemical machine.  At the same time, an increasing number of
	      evolutionary computation domains are evolving non-parameterized
	      mechanisms such as neural networks or symbolic computer programs.
	      As such, we think modern biological genetics offers much in helping
	      us understand how to evolve such things.  In this paper, we present
	      a gene regulation model for <i>Drosophila melanogaster</i>.  We
	      then apply gene regulation to evolve deterministic finite-state
	      automata, and show that our approach does well compared to past
	      examples from the literature.
       </dl>


       <a name="ARevisedComparisonOfCrossoverAndMutation"></a><tr><td class="dte">A Revised Comparison of Crossover and Mutation in Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and Lee Spector -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/revisedgp98.pdf">PDF</a>
	 <dt><i>Note:</i><dd>
	      This paper is a revision of a <a href="index.html#AComparisonOfCrossoverAndMutation">previous paper</a>, with statistical correction and a considerable new set of data.  However, the original also has some data that does not appear here, so you may want to consider getting both.
	 <dt><i>Citation:</i><dd>Sean Luke and Lee Spector.  1998.  A Revised Comparison of Crossover and Mutation in Genetic Programming.  In <i>Proceedings of the Third Annual Genetic Programming Conference (GP98)</i>.  J. Koza <i>et al</i>, eds.  208-213.  San Fransisco: Morgan Kaufmann.

	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b5');">Show / Hide</a><dd id='b5' style="display:none;">
	     In <a href="index.html#AComparisonOfCrossoverAndMutation">[Luke and Spector 1997]</a> we presented a comprehensive suite of data
	      comparing GP crossover and point mutation over four domains and a wide
	      range of parameter settings.  Unfortunately, the results were marred by
	      statistical flaws.  This revision of the study eliminates these flaws,
	      with three times as much the data as the original experiments had.  Our
	      results again show that crossover does have some advantage over mutation
	      given the right parameter settings (primarily larger population sizes),
	      though the difference between the two surprisingly small.  Further, the
	      results are complex, suggesting that the big picture is more complicated
	      than is commonly believed.
       </dl>
       
       
     
  <a name="AComparisonOfCrossoverAndMutation"></a><tr><td class="dte">A Comparison of Crossover and Mutation in Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and Lee Spector -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/comparison/comparison.pdf">PDF</a>
        <dt><i>Note:</i><dd>This paper has been superceeded by <a HREF="index.html#ARevisedComparisonOfCrossoverAndMutation">A Revised Comparison of Crossover and Mutation in Genetic Programming</a>.  The revised version has new data which corrects some statistical flaws in the original.
	 <dt><i>Citation:</i><dd>Sean Luke and Lee Spector. 1997. A Comparison of Crossover and Mutation in Genetic Programming.
	      In <i>Genetic Programming 1997: Proceedings of the Second Annual Conference (GP97)</i>.
	      J. Koza <i>et al</i>, eds.  San Fransisco: Morgan Kaufmann.  240-248.
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b6');">Show / Hide</a><dd id='b6' style="display:none;">
	      This paper presents a large and systematic body of data on the relative
	      effectiveness of mutation, crossover, and combinations of mutation and
	      crossover in genetic programming (GP). The literature of traditional
	      genetic algorithms contains related studies, but mutation and crossover in
	      GP differ from their traditional counterparts in significant ways.  In
	      this paper we present the results from a very large experimental data set,
	      the equivalent of approximately 12,000 typical runs of a GP system,
	      systematically exploring a range of parameter settings.  The resulting
	      data may be useful not only for practitioners seeking to optimize
	      parameters for GP runs, but also for theorists exploring issues such as
	      the role of "building blocks" in GP.
       </dl>
       
  <a name="teamwork"></a><tr><td class="dte">
       Evolving Teamwork and Coordination with Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and Lee Spector -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/cooperation.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke and Lee Spector. 1996. Evolving Teamwork and Coordination with Genetic Programming.  In <i>Genetic Programming 1996: Proceedings of the First Annual Conference.</i>. J. Koza <i>et al</i>, eds.  Cambridge: MIT Press. 141-149.	
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b6a');">Show / Hide</a><dd id='b6a' style="display:none;">Some problems can be solved only by multi-agent teams. In using genetic
	      programming to produce such teams, one faces several design decisions.
	      First, there are questions of team diversity and of breeding strategy.
	      In one commonly used scheme, teams consist of clones of single individuals;
	      these individuals breed in the normal way and are cloned to form
	      teams during fitness evaluation. In contrast, teams could also consist of
	      distinct individuals. In this case one can either allow free interbreeding
	      between members of different teams, or one can restrict interbreeding
	      in various ways. A second design decision concerns the types of
	      coordination-facilitating mechanisms provided to individual team members;
	      these range from sensors of various sorts to complex communication systems.
	      This paper examines three breeding strategies (clones, free, and restricted)
	      and three coordination mechanisms (none, deictic sensing, and name-based
	      sensing) for evolving teams of agents in the Serengeti world, a simple
	      predator/prey environment. Among the conclusions are the fact that
	      a simple form of restricted interbreeding outperforms free interbreeding
	      in all teams with distinct individuals, and the fact that name-based
	      sensing consistently outperforms deictic sensing.
       </dl>
       
	  
  <a name="CulturalTransmissionOfInformationInGeneticProgramming"></a><tr><td class="dte">Cultural Transmission of Information in Genetic Programming</a></td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Lee Spector and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/culture-gp96.pdf">PDF</a>
	<dt><i>Note:</i><dd>
	Discussion of this paper appeared as part of the <i>Scientific American</i> (Oct. 96) column "Computing: Programming with Primordial Ooze" (p. 50). 	
	 <dt><i>Citation:</i><dd>Lee Spector and Sean Luke. 1996. Cultural Transmission of Information in Genetic Programming.  In <i>Genetic Programming 1996: Proceedings of the First Annual Conference.</i>. J. Koza <i>et al</i>, eds.  Cambridge: MIT Press. 200-208.	
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b7');">Show / Hide</a><dd id='b7' style="display:none;">
	      This paper shows how the performance of a genetic programming system
	      can be improved through the addition of mechanisms for non-genetic
	      transmission of information between individuals (culture). Teller
	      has previously shown how genetic programming systems can be enhanced
	      through the addition of memory mechanisms for individual programs
	      [Teller 1994]; in this paper we show how Teller's memory mechanism
	      can be changed to allow for communication between individuals within
	      and across generations. We show the effects of indexed memory and
	      culture on the performance of a genetic programming system on a symbolic
	      regression problem, on Koza's Lawnmower problem, and on Wumpus world
	      agent problems. We show that culture can reduce
	      the computational effort required to solve all of these problems.
	      We conclude with a discussion of possible improvements.
	</dl>
	
	
 <a name="CultureEnhancesTheEvolvabilityOfCognition"></a><tr><td class="dte">Culture Enhances the Evolvability of Cognition</td><td class="dde">
<dl>
	 <!--<dt><i>Authors:</i><dd>Lee Spector and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/culture-cogsci.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Lee Spector and Sean Luke. 1996. Culture Enhances the Evolvability of Cognition.  In <i>Cognitive Science 1996 Conference Proceedings (CogSci96)</i>.
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b8');">Show / Hide</a><dd id='b8' style="display:none;">This paper discusses the role of culture in the evolution of cognitive
	      systems. We define "culture" as any information transmitted between
	      individuals and between generations by non-genetic means. Experiments
	      are presented that use genetic programming systems that include special
	      mechanisms for cultural transmission of information. These systems
	      evolve computer programs that perform cognitive tasks including mathematical
	      function mapping and action selection in a virtual world. The data show
	      that the presence of culture-supporting mechanisms can have a clear
	      beneficial impact on the evolvability of correct programs. The implications
	      that these results may have for cognitive science are
	      briefly discussed.
       </dl>






	  
  <a name="EvolvingGraphsAndNetworksWithEdgeEncodingPreliminaryReport"></a><tr><td class="dte">Evolving Graphs and Networks with Edge Encoding: Preliminary Report</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Sean Luke and Lee Spector -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <A HREF="https://cs.gmu.edu/~sean/papers/graph-paper.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke and Lee Spector. 1996. Evolving Graphs and Networks with Edge encoding: Preliminary Report.  In <i>Late Breaking Papers at the Genetic Programming 1996 Conference (GP96)</i>. J. Koza, ed.  Stanford: Stanford Bookstore.  117-124.	
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b9');">Show / Hide</a><dd id='b9' style="display:none;">
	      We present an alternative to the cellular encoding
	      technique for evolving graph and network structures
	      via genetic programming. The new technique, called
	      edge encoding, uses edge operators rather than the node operators
	      of cellular encoding. While both cellular encoding and edge encoding
	      can produce all possible graphs, the two encodings bias
	      the genetic search process in different ways; each may therefore be most
	      useful for a different set of problems. The problems for which
	      these techniques may be used, and for which we think edge encoding may
	      be particularly useful, include the evolution of recurrent neural
	      networks, finite automata, and graph-based queries to symbolic knowledge
	      bases. In this preliminary report we present a technical description of
	      edge encoding and an initial comparison to cellular encoding.
	      Experimental investigation of the relative merits of these encoding
	      schemes is currently in progress.
       </dl>
       















           <tr><td colspan=2 class="spread"><a name="CodeBloat"></a>Code Bloat</td>
	  </tr>


	  	                    
	                      <a name="AComparisonOfBloatControlMethodsForGeneticProgramming"></a><tr><td class="dte">A Comparison of Bloat Control Methods for Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and Liviu Panait -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/parsimony.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke and Liviu Panait. 2006. A Comparison of Bloat Control Methods for Genetic Programming.  <i>Evolutionary Computation</i>.  14(13):309-344. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h2');">Show / Hide</a><dd id='h2' style="display:none;"> Genetic programming has highlighted the problem of bloat, the uncontrolled growth of the average size of an individual in the population. The most common approach to dealing with bloat in tree-based genetic programming individuals is to limit their maximal allowed depth. An alternative to depth limiting is to punish individuals in some way based on excess size, and our experiments have shown that the combination of depth limiting with such a punitive method is generally more effective than either alone. Which such combinations are most effective at reducing bloat? In this article we augment depth limiting with nine bloat control methods and compare them with one another. These methods are chosen from past literature and from techniques of our own devising. Testing with four genetic programming problems, we identify where each bloat control method performs well on a per-problem basis, and under what settings various methods are effective independent of problem. We report on the results of these tests, and discover an unexpected winner in the cross-platform category. 
       </dl>

  
                            <a name="AlternativeBloatControlMethods"></a><tr><td class="dte">Alternative Bloat Control Methods</td><td class="dde">
       <dl>
	      <a href="https://cs.gmu.edu/~sean/papers/gecco04-parsimony.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Liviu Panait and Sean Luke. 2004. Alternative Bloat Control Methods.  In <i>Genetic and Evolutionary Computation Conference (GECCO)</i>.  Springer.  Pages 630-641.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f6');">Show / Hide</a><dd id='f6' style="display:none;">Bloat control is an important aspect of evolutionary computation methods, such as genetic programming, which must deal with genomes of arbitrary size. We introduce three new methods for bloat control: Biased Multi-Objective Parsimony Pressure (BMOPP), the Waiting Room, and Death by Size. These methods are unusual approaches to bloat control, and are not only useful in various circumstances, but two of them suggest novel approaches to attack the problem. BMOPP is a more traditional parsimony-pressure style bloat control method, while the other two methods do not consider parsimony as part of the selection process at all, but instead penalize for parsimony at other stages in the evolutionary process. We find parameter settings for BMOPP and the Waiting Room which are effective across all tested problem domains. Death by Size does not appear to have this consistency, but we find it a useful tool as it has particular applicability to steady-state evolution. 
       </dl>

  <a name="ModificationPointDepthAndGenomeGrowthInGeneticProgramming"></a><tr><td class="dte">Modification Point Depth and Genome Growth in Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/depth.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke. 2003. Modification Point Depth and Genome Growth in Genetic Programming.  <i>Evolutionary Computation</i>.  11(1):67-106. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('i0');">Show / Hide</a><dd id='i0' style="display:none;"> The evolutionary computation community has shown increasing interest in arbitrary-length representations, particularly in the field of genetic programming.  A serious stumbling block to the scalability of such representations has been {\it bloat}: uncontrolled genome growth during an evolutionary run.  Bloat appears across the evolutionary computation spectrum, but genetic programming has given it by far the largest attention.  Most genetic programming models explain this phenomenon as a result of the growth of {\it introns}, areas in an individual which serve no functional purpose.  This paper presents evidence which directly contradicts intron theories.  The paper then uses data drawn from this evidence to propose a new model of genome growth.  In this model, bloat in genetic programming is a function of the mean depth of the modification (crossover or mutation) point.  Points far from the root are correspondingly less likely to hurt the child's survivability in the next generation.  The modification point is in turn strongly correlated to average parent tree size and to removed subtree size, both of which are directly linked to the size of the resulting child.

       </dl>



       <a name="FightingBloatWithNonparametricParsimonyPressure"></a><tr><td class="dte">Fighting Bloat With Nonparametric Parsimony Pressure</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Sean Luke and Liviu Panait -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/parsimony2.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Sean Luke and Liviu Panait.  2002.  Fighting Bloat With Nonparametric Parsimony Pressure.  In <i>Parallel Problem Solving from Nature - PPSN VII (LNCS 2439)</i>.  Juan Julian Merelo Guervos <i>et al</i>, eds.  Springer Verlag.  411-421.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('b0');">Show / Hide</a><dd id='b0' style="display:none;">
Many forms of parsimony pressure are parametric, that is final fitness is a parametric model of the actual size and raw fitness values.  The problem with parametric techniques is that they are hard to tune to prevent size from dominating fitness late in the evolutionary run, or to compensate for problem-dependent nonlinearities in the raw fitness function.  In this paper we briefly discuss existing bloat-control techniques, then introduce two new kinds of non-parametric parsimony pressure, Direct and Proportional Tournament.  As their names suggest, these techniques are based on simple modifications of tournament selection to consider both size and fitness, but not together as a combined parametric equation.  We compare the techniques against, and in combination with, the most popular genetic programming bloat-control technique, Koza-style depth limiting, and show that they are effective in limiting size while still maintaining good best-fitness-of-run results.
      </dl>
      								

	<a name="LexicographicParsimonyPressure"></a><tr><td class="dte">Lexicographic Parsimony Pressure</td><td class="dde">
	     <dl>
	<!--<dt><i>Authors: </i><dd>Sean Luke and Liviu Panait -->
	<!--<dt><i>Formats: </i><dd>-->
	     <a href="https://cs.gmu.edu/~sean/papers/lexicographic.pdf">PDF</a>
	<dt><i>Citation: </i><dd>Sean Luke and Liviu Panait.  2002.  Lexicographic Parsimony Pressure.  In <i>GECCO-2002: Proceedings of the Genetic and Evolutionary Computation Conference</i>.  W. B. Langdon <i>et al</i>, eds.  Morgan Kauffman.  829-836.
	<dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c1');">Show / Hide</a><dd id='c1' style="display:none;">
We introduce a technique called lexicographic parsimony pressure, for controlling the significant growth of genetic programming trees during the course of an evolutionary computation run.  Lexicographic parsimony pressure modifies selection to prefer smaller trees only when fitnesses are equal (or equal in rank).  This technique is simple to implement and is not affected by specific differences in fitness values, but only by their relative ranking.  In two experiments we show that lexicographic parsimony pressure reduces tree size while maintaining good fitness values, particularly when coupled with Koza-style maximum tree depth limits.
      </dl>
      								


     <a name="CodeGrowthIsNotCausedByIntrons"></a><tr><td class="dte">Code Growth Is Not Caused By Introns</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/intronpaper.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke.  2000.  Code growth is not caused by introns.  In <i>Late Breaking Papers at the 2000 Genetic and Evolutionary Computation Conference</i> (GECCO-2000).  228-235.  
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c2');">Show / Hide</a><dd id='c2' style="display:none;">
Genetic programming trees have a strong tendency to grow rapidly and relatively independent of fitness, a serious flaw which has received considerable attention in the genetic programming literature.  Much of this literature has implicated <i>introns</i>, subtree structures with no effect on the an individual's fitness assessment.  The propagation of <i>inviable code</i>, a certain kind of intron, has been especially linked to tree growth.  However this paper presents evidence which shows that denying inviable code the opportunity to propagate actually increases tree growth.  The paper argues that rather than causing tree growth, a rise in inviable code is in fact an expected result of tree growth.  Lastly, this paper proposes a more general theory of growth for which introns are merely a symptom.

       </dl>
    
       




	<tr><td colspan=2 class="spread"><a name="Robotics"></a>Robotics</td>
	  </tr>

 <a name="PortableSensorMotesAsADistributedCommunicationMediumForLargeGroupsOfMobileRobots"></a>
<tr><td class="dte">Portable Sensor Motes as a Distributed Communication Medium for Large Groups of Mobile Robots</td><td class="dde"><dl>  
              <a href="https://cs.gmu.edu/~sean/papers/SET222Luke.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Sean Luke and Katherine Russell.  2016.  Portable Sensor Motes  as a Distributed Communication Medium for Large Groups of Mobile Robots. In <i>NATO Specialists Meeting on Swarm Centric Solution for Intelligent Sensor Networks (SET-222).</i>  
        <dt><i>Note: </i><dd>This is a short position paper summarizing our past work on using sensor motes as beacons for pheromone robotics.  
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e92');">Show / Hide</a><dd id='e92' style="display:none;">  
We argue for the use of swarms of distributed portable sensors as a support medium for a large number of autonomous mobile robots.  Because of the scaling issues inherent in their multiplicity, and because they may operate in broadcast-denied environments, swarm robot architectures often focus on local and &lquot;indirect&rquo; communication methods such as breadcrumbs, pheromones, or messages left in the environment.  We are interested in how far we can go with these models in real robots.  To this end, our research investigates robots capable of deploying, retrieving, moving, and locally communicating with many embedded sensor motes.  The mobile agents deploy and optimize the location of the motes, read historic and current sensor data from them, and store useful local information in them for other mobile agents to discover later.  We have demonstrated the ability to do robot foraging in environments with significant noise and physical disruption, such as might occur in any deployment of a large sensor network. We have also demonstrated experiments using swarms and sensor motes to collectively build sophisticated, non-trivial swarm behaviors, such as laying out complex shapes using compass/straightedge geometry.  In this paper we discuss these results and their limitations, and indicate where we think wireless sensor mote technology can help advance swarm robotics going forward.  </dl>

 <a name="SupportingMobileSwarmRoboticsInLowPowerAndLossySensorNetworks"></a>
<tr><td class="dte">Supporting Mobile Swarm Robotics in Low Power and Lossy Sensor Networks</td><td class="dde">
       <dl>  
              <a href="https://cs.gmu.edu/~sean/papers/SET222Andrea.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Kevin Andrea, Robert Simon, and Sean Luke.  2016.  Supporting Mobile Swarm Robotics in Low Power and Lossy Sensor Networks.  In <i>NATO Specialists Meeting on Swarm Centric Solution for Intelligent Sensor Networks (SET-222).</i>  
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e93');">Show / Hide</a><dd id='e93' style="display:none;">  
Wireless low-power and lossy networks (LLNs) are a key enabling technology for the deployment of massively scaled self-organizing sensor swarm systems.  Supporting applications such as providing human users situational awareness across large areas requires that swarm-friendly LLNs effectively support communication between embedded and mobile devices, such as autonomous robots.  The reason for this is that large scale embedded sensor applications such as unattended ground sensing systems typically do not have full end-to-end connectivity, but suffer frequent communication partitions.  Further, it is desirable for many tactical applications to offload tasks to mobile robots.  Despite the importance of support this communication pattern, there has been relatively little work in designing and evaluating LLN-friendly protocols capable of supporting such interactions.  

<p>This paper addresses the above problem by describing the design, implementation, and evaluation of the MoRoMi system.  MoRoMi stands for Mobile Robotic MultI-sink.  It is intended to support autonomous mobile robots that interact with embedded sensor swarms engaged in activities such as cooperative target observation, collective map building, and ant foraging.  These activities benefit if the swarm can dynamically interact with embedded sensing and actuator systems that provide both local environmental or positional information and an ad-hoc communication system.  Our paper quantifies the performance levels that can be expected using current swarm and LLN technologies.  
</dl>




<tr><td class="dte">
              <a name="SwarmRobotForagingwithWirelessSensorMotes"></a>Swarm Robot Foraging with Wireless Sensor Motes</td><td class="dde">
 <dl>
              <a href="https://cs.gmu.edu/~sean/papers/beacons14.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Katherine Russell, Michael Schader, Kevin Andrea, and Sean Luke.  2015.  Swarm Robot Foraging with Wireless Sensor Motes.  In <i>International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2015)</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('j160');">Show / Hide</a><dd id='j160' style="display:none;">
We investigate the use of wireless sensor motes as mobile deployable waypoints for swarm robot navigation in a foraging scenario. Each robot can deploy, retrieve, and optimize the location of the  sensor motes. After deployment, the robots treat the sensor motes as nodes in a sparse graph, and store and retrieve multiple pheromones and flags locally in each of them. Pheromone information stored in the sensor motes allows the robots to build up gradients to different targets of interest, and to determine which sensor motes are good candidates to optimize location, or to harvest for reuse elsewhere. Unlike many earlier pheromone-based foraging techniques, our method must deal with the physical reality of deploying and manipulating sensor motes, including a limited mote supply both onboard and in total, robustly dealing with occlusion and interference from other robots, and handling noise and robot or mote failure. We demonstrate the effectiveness of the technique both on differential drive robots of our own design, and in simulation, to examine its ability to robustly deal with various failure modes and changes in environment.
</dl>


<tr><td class="dte">
              <a name="TrainingHeterogeneousTeamsOfRobots"></a>Training Heterogeneous Teams of Robots</td><td class="dde">
 <dl>
              <a href="https://cs.gmu.edu/~sean/papers/arms15.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Ermo Wei, Bill Squires, Drew Wicke, and Sean Luke.  2015.  Training Heterogeneous Teams of Robots
  In <i>Autonomous Robots and Multibot Systems Workshop (ARMS)</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('j150');">Show / Hide</a><dd id='j150' style="display:none;">
Heterogeneous multi-robot teams are common solutions to complex tasks, especially those that are inherently cooperative.  Training robots, rather than coding them, to work together in these teams is an attractive prospect, but is very difficult due to the extremely large state space and the inherent inverse problem which separates the agents&rsquo; micro-level behaviors and the desired macro-level emergent phenomenon.  We approach this problem with HiTAB, a learning from demonstration system which uses behavior decomposition to allow rapid training of teams with minimal samples.  This paper presents and compares two approaches to training teams of heterogenous robots: first, forming a multiagent control hierarchy which scales to large numbers of robots but requires the training of additional virtual controller agents; and second, modifying each robot&rsquo;s feature space to include information about other robots&rsquo; current behaviors or limited internal state.
</dl>

<tr><td class="dte">
              <a name="RoboPatriotsGeorgeMasonUniversity2015RoboCupTeam"></a>RoboPatriots: George Mason University 2015 RoboCup Team</td><td class="dde">
 <dl>
              <a href="https://cs.gmu.edu/~sean/papers/robocup15tdb.pdf">PDF</a>
         <dt><i>Citation:</i><dd>David Freelan, Drew Wicke, Chris Burns, Carl Walker, Laura Hovatter, Colin Ward, Daniel Lofaro, and Sean Luke. 2015.  RoboPatriots: George Mason University 2015 RoboCup Team.
  In <i>Proceedings of the 2015 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('j15');">Show / Hide</a><dd id='j15' style="display:none;">
The RoboPatriots are a team of four DARwIn-OP robots from George Mason University which participate in the Kid-Size Humanoid League.  RoboCup 2015 marks the sixth year of participation for the RoboPatriots.  Our approach is very unusual in that our goal is to train our robots how to play cooperative soccer, rather than program them.  We do this not at our laboratory, but at the RoboCup venue during the preparatory period. Then, we enter the learned robot behaviors into the competition.  In 2014 we trained all three attackers, pairing them with a hard-coded goalie.  This year we intend to train a full team of all four robots.
</dl>


<tr><td class="dte">
              <a name="TowardsRapidMultiRobotLearningFromDemonstrationAtTheRoboCupCompetition"></a>Towards Rapid Multi-robot Learning from Demonstration at the RoboCup Competition</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/robocup14.pdf">PDF</a>
         <dt><i>Citation:</i><dd>David Freelan, Drew Wicke, Keith Sullivan, and Sean Luke. 2014.  Towards Rapid Multi-robot Learning from Demonstration at the RoboCup Competition. In <i>Proceedings of the 2014 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('j14');">Show / Hide</a><dd id='j14' style="display:none;">
We describe our previous and current efforts towards achieving an unusual personal RoboCup goal: to train a full team of robots directly through demonstration, on the field of play at the RoboCup venue, how to collaboratively play soccer, and then use this trained team in the competition itself.  Using our method, HiTAB, we can train teams of collaborative agents via demonstration to perform nontrivial joint behaviors in the form of hierarchical finite-state automata.  We discuss HiTAB, our previous efforts in using it in RoboCup 2011 and 2012, recent experimental work, and our current efforts for 2014, then suggest a new RoboCup Technical Challenge problem in learning from demonstration.
</dl>


<tr><td class="dte">
              <a name="RoboPatriotsGeorgeMasonUniversity2014RoboCupTeam"></a>RoboPatriots: George Mason University 2014 RoboCup Team</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/robocup14tdb.pdf">PDF</a>
         <dt><i>Citation:</i><dd>David Freelan, Drew Wicke, Chau Thai, Joshua Snider, Anna Papadogiannakis, and Sean Luke. 2014.  RoboPatriots: George Mason University 2014 RoboCup Team.
In <i>Proceedings of the 2014 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('j15');">Show / Hide</a><dd id='j15' style="display:none;">
The RoboPatriots are a team of four DARwIn-OP robots from George Mason University which participate in the Kid-Size Humanoid League.  RoboCup 2014 marks the fifth year of participation for the RoboPatriots: in 2009 and 2010, we advanced to the second round, and in 2011 we were eliminated in the first round.  Our approach is unusual in that we aim to train our robots how to play soccer using learning from demonstration, then field those robots in the competition.
</dl>


<tr><td class="dte">
              <a name="RealTimeTrainingOfTeamSoccerBehaviors"></a>Real-Time Training of Team Soccer Behaviors</td><td class="dde">
       <dl>
         <!--<dt><i>Authors:</i><dd>Keith Sullivan and Sean Luke -->
         <!--<dt><i>Formats:</i><dd>-->
              <a href="https://cs.gmu.edu/~sean/papers/robocup12training.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan and Sean Luke.  2012.  Real-Time Training of Team Soccer Behaviors.
In <i>Proceedings of the 2012 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('y4');">Show / Hide</a><dd id='y4' style="display:none;">
Training robot or agent behaviors by example is an attractive alternative to directly coding them. However training complex behaviors can be challenging, particularly when it involves interactive behaviors involving multiple agents. We present a novel hierarchical learning from demonstration system which can be used to train both single-agent and scalable cooperative multiagent behaviors. The methodology applies manual task decomposition to break the complex training problem into simpler parts, then solves the problem by iteratively training each part. We discuss our application of this method to multiagent problems in the humanoid RoboCup competition, and apply the technique to the keepaway soccer problem in the RoboCup Soccer Simulator.</dl>

<tr><td class="dte">
              <a name="RoboPatriotsGeorgeMasonUniversity2012RoboCupTeam"></a>RoboPatriots: George Mason University 2012 RoboCup Team</td><td class="dde">
       <dl>
         <!--<dt><i>Authors:</i><dd>Keith Sullivan, Brian Davidson, Christopher Vo, Brian Hrolenok, and Sean Luke -->
         <!--<dt><i>Formats:</i><dd>-->
              <a href="https://cs.gmu.edu/~sean/papers/robocup12tdp.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Katherine Russell, Kevin Andrea, Barak Stout, and Sean Luke.  2012.  RoboPatriots: George Mason University 2012 RoboCup Team.
In <i>Proceedings of the 2012 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('y1');">Show / Hide</a><dd id='y1' style="display:none;">
The RoboPatriots from George Mason University are a team of three humanoid robots. As we are interested in embodied AI, our robots are based on commercially available equipment. We use the three on-board computers for research into learning from demonstration, a supervised machine learning technique for training robot behavior. RoboCup 2012 marks the fourth year of participation for the RoboPatriots: in 2009 and 2010, we advanced to the second round, and in 2011 we were eliminated in the first round.  
</dl>

<tr><td class="dte">
              <a name="LearningFromDemonstrationWithSwarmHierarchies"></a>Learning from Demonstration with Swarm Hierarchies</td><td class="dde">
       <dl>
         <!--<dt><i>Authors:</i><dd>Keith Sullivan and Sean Luke -->
         <!--<dt><i>Formats:</i><dd>-->
              <a href="https://cs.gmu.edu/~sean/papers/aamas12.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan and Sean Luke.  2012.  Learning from Demonstration with Swarm Hierarchies.
In <i>Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('y3');">Show / Hide</a><dd id='y3' style="display:none;">
We present a supervised learning from demonstration system capable of training stateful and recurrent collective behaviors for multiple agents or robots. A model space of this kind is often high-dimensional and consequently may require a large number of samples to learn. Furthermore, the inverse problem posed by emergent macrophenomena among multiple agents presents major challenges to supervised learning methods. Our approach reduces the size of the state space, and shortens the gap between individual behaviors and macrophenomena, by manually decomposing individual behaviors and arranging the agents into a tree hierarchy. This makes it possible to train potentially large numbers of agents using a small number of samples. We demonstrate our system using hundreds of agents in a simulated foraging task, and on real robots performing a collective patrolling task.
</dl>

<tr><td class="dte">
              <a name="RoboPatriotsGeorgeMasonUniversity2011RoboCupTeam"></a>RoboPatriots: George Mason University 2011 RoboCup Team</td><td class="dde">
       <dl>
         <!--<dt><i>Authors:</i><dd>Keith Sullivan, Brian Davidson, Christopher Vo, Brian Hrolenok, and Sean Luke -->
         <!--<dt><i>Formats:</i><dd>-->
              <a href="https://cs.gmu.edu/~sean/papers/robocup11tdp.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Christopher Vo, and Sean Luke.  2011.  RoboPatriots: George Mason University 2011 RoboCup Team
In <i>Proceedings of the 2011 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('z2');">Show / Hide</a><dd id='z2' style="display:none;">
The RoboPatriots are a team of three humanoid robots designed by the Computer Science Department at George Mason University.  Each robot is based on the Kondo KHR-3HV, a customized Surveyor SVS camera, and a Gumstix embedded computer (see Figure 1(a)).  
</dl>

<tr><td class="dte">
     <a name="HierarchicalLearningFromDemonstrationOnHumanoidRobots"></a>Hierarchical Learning from Demonstration on Humanoid Robots</td><td class="dde">
       <dl>
         <a href="https://cs.gmu.edu/~sean/papers/humanoids10hierarchical.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Sean Luke, and Vittorio Ziparo.  2010. Hierarchical Learning from Demonstration on Humanoid Robots. In <i>Proceedings of the Humanoid Robots Learning from Interaction Workshop, at Humanoids 2010.</i>
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h5');">Show / Hide</a><dd id='h5' style="display:none;">
        Developing behaviors for humanoid robots is difficult due to the high complexity of programming these robots, which includes repeated trial and error cycles.  We have recently developed a learning from demonstration system capable of training agent behaviors from a small number of training examples.  Our system represents a complex behavior as a hierarchical finite automaton, permitting decomposition of the behavior into simple, easier-to-train sub-behaviors.  The system was originally designed for swarms of virtual agents, but based on recent Robocup experience, we have ported the system to our humanoid robot platform.  We discuss the system and the platform, and preliminary experiments involving both novice and expert users in a stateful visual servoing task.   
</dl>


<tr><td class="dte">
              <a name="RoboPatriotsGeorgeMasonUniversity2010RoboCupTeam"></a>RoboPatriots: George Mason University 2010 RoboCup Team</td><td class="dde">
       <dl>
              <a href="https://cs.gmu.edu/~sean/papers/robocup10tdp.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Keith Sullivan, Christopher Vo, Sean Luke, and Jyh-Ming Lien.  2010.  RoboPatriots: George Mason University 2010 RoboCup Team
In <i>Proceedings of the 2010 RoboCup Workshop</i>.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('z20');">Show / Hide</a><dd id='z20' style="display:none;">
The RoboPatriots are a team of three humanoid robots sponsored by the Computer Science Department at George Mason University. Each robot is based on the Kondo KHR-3HV and a customized Surveyor SVS camera.
</dl>



<tr><td class="dte">
     <a name="LearnToBehaveRapidTrainingOfBehaviorAutomata"></a>Learn to Behave!  Rapid Training of Behavior Automata</td><td class="dde">
       <dl>
         <a href="https://cs.gmu.edu/~sean/papers/aamas10-learntobehave.pdf">PDF</a>
         <dt><i>Citation:</i><dd>Sean Luke and Vittorio Ziparo.  2010.  Learn to Behave!  Rapid Training of Behavior Automata.  In <i>Proceedings of the Adaptive and Learning Agents Workshop at AAMAS 2010</i>.  Marek Grze&#x015B; and Matthew Taylor, editors.   61&ndash;68.
         <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e6');">Show / Hide</a><dd id='e6' style="display:none;">
       Programming robot or virtual agent behaviors can be a challenging task, and makes attractive the prospect of automatically learning the behaviors from the actions of a human demonstrator.  However, learning complex behaviors rapidly from a demonstrator may be difficult if they demand a large number of training samples.  We describe an architecture for rapid learning of recurrent behaviors from demonstration.  The architecture is based on deterministic hierarchical finite-state automata (HFAs) with classification algorithms taking the place of the state transition function.  This architecture allows for task decomposition, statefulness, parameterized features and behaviors, per-behavior feature set customization, and storage of learned behaviors in libraries to be used later on as elements in more complex behaviors.  We describe the system, then illustrate its application in a simple, but nontrivial,  foraging task involving multiple behaviors.
	</dl>




              <a name="RoboPatriotsGeorgeMasonUniversity2009RoboCupTeam"></a><tr><td class="dte">RoboPatriots: George Mason University 2009 RoboCup Team</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Keith Sullivan, Brian Davidson, Christopher Vo, Brian Hrolenok, and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/robocup09tdp.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Keith Sullivan, Brian Davidson, Christopher Vo, Brian Hrolenok, and Sean Luke.  2009.  RoboPatriots: George Mason University 2009 RoboCup Team.  In <i>Proceedings of the 2009 RoboCup Workshop</i>. 
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e5');">Show / Hide</a><dd id='e5' style="display:none;">
The RoboPatriots are a team of three humanoid robots sponsored by George 
Mason University. Each robot is built on a Kondo KHR-1HV humanoid base and 
a customized Gumstix Verdex microcontroller. The two attackers share the same 
behavior, and the goalie's is different. At present there is minimal communication 
between the robots, but this may change by the competition. 

	   <P>
	   RoboCup 2009 provides an opportunity for the Autonomous Robotics Lab 
at George Mason University to apply its expertise in evolutionary algorithms 
to robotics. Our research goals in developing the RoboPatriots have focused on 
three elements: first, we are interested in the multiagent coordination aspects of 
the problem. Second, we are simplifying the design of robot motion behaviors by 
gathering human motion data, then converting this data to the humanoid servos. 
Third, we are optimizing these motion behaviors in a massively distributed evo- 
lutionary computation system attached to a custom humanoid robot simulator 
of our own devising.        </dl>



             

              <a name="RoboPatriotsGeorgeMasonUniversity2008RoboCupTeam"></a><tr><td class="dte">RoboPatriots: George Mason University 2008 RoboCup Team</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Keith Sullivan, Brian Davidson, Christopher Vo, Brian Hrolenok, and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/robopatriots2008.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Keith Sullivan, Brian Davidson, Christopher Vo, Brian Hrolenok, and Sean Luke.  2008.  RoboPatriots: George Mason University 2008 RoboCup Team.  In <i>Proceedings of the 2008 RoboCup Workshop</i>. 
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('e3');">Show / Hide</a><dd id='e3' style="display:none;">
The Autonomous Robotics Laboratory was established at George Mason University in 2006 with the goal of collaborative research in robotics, multiagent 
systems, computer vision, and sensor networks. While our previous work has 
focused on differential drive robots, RoboCup 2008 represents our fist major effort with humanoid robots. The goal this year is modest: construct and program 
a working team for RoboCup. Our primary focus is developing a basic hardware 
and software platform to base future research initiatives on. 
RoboPatriots has three robots: two attackers and one goalie. All three have 
the same hardware. The two attackers have one behavior mechanism, and the 
goalie has a different behavior mechanism. Presently, there is no communication 
between the robots, but that may change prior to the competition.
       </dl>




  <a name="ThreeRoboCupSimulationLeagueCommentatorSystems"></a><tr><td class="dte">Three RoboCup Simulation League Commentator Systems</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Elisabeth Andre, Kim Binsted, Kumiko Tanaka-Ishii, Sean Luke, Gerd Herzog, and Thomas Rist -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <A HREF="https://cs.gmu.edu/~sean/papers/commentator.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>E. Elisabeth Andre, Kim Binsted, Kumiko Tanaka-Ishii, Sean Luke, Gerd Herzog, and Thomas Rist.  2000.  Three RoboCup simulation league commentator systems.  In <i>AI Magazine</i>, 21:1 (Spring 2000), 57-66.  AAAI.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c3');">Show / Hide</a><dd id='c3' style="display:none;">
	     Three systems which generate real-time natural language commentary on the RoboCup simulation league are presented, and their similarities, differences, and directions for the future discussed.  Although they emphasize different aspects of the commentary problem, all three systems take simulator data as input, and generate appropriate, expressive, spoken commentary in real time.
       </dl>

	    <a name="CharacterDesignForSoccerCommentary"></a><tr><td class="dte">Character Design for Soccer Commentary</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Kim Binsted and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/byrne.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Kim Binsted and Sean Luke.  1998.  Character design
	      for soccer commentary.  In <i>Robot Soccer World Cup II: Proceedings
	      of the second RoboCup Workshop,</i> H. Kitano, ed.  23-35.  Springer-Verlag.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c4');">Show / Hide</a><dd id='c4' style="display:none;">
	   In this paper we present early work on an animated talking head
	      commentary system called <i>Byrne</i>. The goal of this project is to
	      develop a system which can take the output from the RoboCup soccer
	      simulator, and generate appropriate affective speech and facial
	      expressions, based on the character's personality, emotional state,
	      and the state of play. Here we describe a system which takes
	      pre-analysed simulator output as input, and which generates text
	      marked-up for use by a speech generator and a face animation
	      system. We make heavy use of inter-system standards, so that future
	      versions of Byrne will be able to take advantage of advances in the
	      technologies that it incorporates.
       </dl>
       

  <a name="GeneticProgrammingProducedCompetitiveSoccerSoftbotTeamsForRoboCup97"></a><tr><td class="dte">
       Genetic Programming Produced Competitive Soccer Softbot Teams for RoboCup97</td><td class="dde">
       <dl>
	 <!--<dt><i>Author: </i><dd>Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/robocupgp98.pdf">PDF</a>
	 <dt><i>Note:</i><dd>
	      This paper is similar to an <A HREF="index.html#Soccer">earlier workshop paper</a>.
	      The key difference being that the workshop paper, which was not for a
	      Genetic Programming audience, is short on experimental details and long
	      on introductions to how GP works.  There also exists a
	      <a href="index.html#EvolvingSoccerBotsARetrospective"> short invited paper</a>
	      detailing how this experiment could have been improved.
	      Also available is a <a href="index.html#CoevolvingSoccerSoftbots">short sidebar</a>
	      for an AI Magazine article.
	 <dt><i>Citation:</i><dd>Sean Luke.  1998.  Genetic Programming Produced Competitive Soccer Softbot Teams for RoboCup97.  In <i>Proceedings of the Third Annual Genetic Programming Conference (GP98)</i>.  J. Koza <i>et al</i>, eds.  204-222.  San Fransisco: Morgan Kaufmann.
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c5');">Show / Hide</a><dd id='c5' style="display:none;">
	      At RoboCup, teams of autonomous robots or software softbots compete in
	      simulated soccer matches to demonstrate cooperative robotics techniques in
	      a very difficult, real-time, noisy environment.  At the IJCAI/RoboCup97
	      softbot competition, all entries but ours used human-crafted cooperative
	      decision-making behaviors.  We instead entered a softbot team whose
	      high-level decision making behaviors had been entirely evolved using
	      genetic programming. Our team won its first two games against
	      human-crafted opponent teams, and received the RoboCup Scientific
	      Challenge Award.  This report discusses the issues we faced and the
	      approach we took to use GP to evolve our robot soccer team for this
	      difficult environment.
       </dl>
       
  <a name="EvolvingSoccerBotsARetrospective"></a><tr><td class="dte">Evolving SoccerBots: A Retrospective</td><td class="dde">
       <dl>
	 <!--<dt><i>Author: </i><dd>Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <A HREF="https://cs.gmu.edu/~sean/papers/robocupShort.pdf">PDF</a>
	 <dt><i>Note: </i><dd>This short invited paper was meant to complement the more complete <A href="index.html#GeneticProgrammingProducedCompetitiveSoccerSoftbotTeamsForRoboCup97"> GP98</a> and <a href="index.html#Soccer"> RoboCup97 </a> papers, and an AI Magazine <a href="index.html#CoevolvingSoccerSoftbots">sidebar</a>, by discussing things that could have been improved from our previous attempt. 
	 <dt><i>Citation: </i><dd>Sean Luke. 1998.  Evolving soccerbots: a retrospective.  In <i>Proceedings of 12th Annual Conference of the Japanese Society for Artificial Intelligence (JSAI)</i>.  
	      <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c6');">Show / Hide</a><dd id='c6' style="display:none;">
		In the RoboCup97 robot soccer tournament, we entered a team of softbot programs whose player strategies had been entirely learned by computer.  Our team beat other human-coded competitors and received the RoboCup97 Scientific Challenge award.  This paper discusses our approach, and details various ways that, in retrospect, it could have been improved.
       </dl>
       


	    <a name="Soccer"></a><tr><td class="dte">Co-evolving Soccer Softbot Team Coordination with Genetic Programming</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd> Sean Luke, Charles Hohn, Jonathan Farris, Gary Jackson, and James Hendler -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <!-- The early version in <a href="robocup.pdf">PDF</a><br> -->
	      <!--The later version in --><a href="https://cs.gmu.edu/~sean/papers/robocupc.pdf">PDF</a><!-- <i>(Recommended)</i> -->
	 <dt><i>Note:</i><dd><!--This paper has an early original version written before the project was completed, and a later version written after the project and after the RoboCup97 competition at IJCAI97 in Nagoya, Japan.  The later version is far better, with corrected and new information, but is also much larger byte-wise.  -->Also available is a <a href="index.html#CoevolvingSoccerSoftbots">short sidebar</a> for an AI Magazine article.  There also exists a <a href="index.html#GeneticProgrammingProducedCompetitiveSoccerSoftbotTeamsForRoboCup97">similar paper</a> written for GP97 which discusses more of the project implementation details but spends little time explaining what Genetic Programming is and how it works.  And finally, there exists a <a href="index.html#EvolvingSoccerBotsARetrospective"> short invited paper</a> detailing how this experiment could have been improved.
	<!-- <dt><i>Citation (for the early original paper):</i><dd> Luke, Charles Hohn, Jonathan Farris, Gary Jackson, and James Hendler. 1997. Co-evolving Soccer Softbot Team Coordination with Genetic Programming.  In <i>Proceedings of the RoboCup-97 Workshop at the 15th International Joint Conference on Artificial Intelligence (IJCAI97)</i>. H. Kitano, ed.  IJCAI.  115-118. -->	
	 <dt><i>Citation<!-- (for the later paper -- the preferred citation)-->:</i><dd> Luke, Charles Hohn, Jonathan Farris, Gary Jackson, and James Hendler. 1998. Co-evolving Soccer Softbot Team Coordination with Genetic Programming.  In <i>RoboCup-97: Robot Soccer World Cup I (Lecture Notes in Artificial Intelligence No. 1395),</i> H. Kitano, ed.  Berlin: Springer-Verlag.  398-411.
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c7');">Show / Hide</a><dd id='c7' style="display:none;">
Genetic Programming is a promising new method for automatically generating functions and algorithms through natural selection.  In contrast to other learning methods, Genetic Programming's automatic programming makes it a natural approach for developing algorithmic robot behaviors. In this paper we present an overview of how we apply Genetic Programming to behavior-based team coordination in the RoboCup Soccer Server domain.  The result is not just a hand-coded soccer algorithm, but a team of softbots which have <i>learned on their own</i> how to play a reasonable game of soccer. 
       </dl>
       
  <a name="CoevolvingSoccerSoftbots"></a><tr><td class="dte">Coevolving Soccer Softbots</td><td class="dde">
	    <dl>
	      <!--<dt><i>Author:</i><dd>Sean Luke -->
		   <!--<dt><i>Formats:</i><dd>-->
			<a href="https://cs.gmu.edu/~sean/papers/robocup3.pdf">PDF</a>
	      <dt><i>Note:</i><dd>This is a short sidebar about the RoboCup project.  A more complete paper can be found <a href="index.html#GeneticProgrammingProducedCompetitiveSoccerSoftbotTeamsForRoboCup97">here</a>.
		   <dt><i>Citation:</i><dd>Sean Luke.  1998.  Coevolving Soccer Softbots.  In <i>AI Magazine</i> 19:3 (Fall 1998), pp. 54.
		   <dt><i>First Paragraph:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d8a');">Show / Hide</a><dd id='d8a' style="display:none;">
		     The University of Maryland RoboCup simulator entry (Sean Luke, Charles hohn, Jonathan Farris, Gary Jackson, and James Hendler) consisted entirely of computer-<i>evolved</i> players developed with Genetic Programming.  Genetic Programming is a branch of evolutionary computation which uses natural selection to optimize over the space of computer algorithms.  Unlike other entrants who fashioned fine-tuned softbot teams from a battery of relatively well-understood robotics techniques, maryland's goal was to see if it was even <i>possible</i> to use evolutionary computation to develop high-level soccer behaviors that were competitive with the human-crafted strategies of other teams.  While evolutionary computation has been successful in many fields, evolving a computer algorithm has proven challenging, especially in a domain like robot soccer.
		 </dl>
		 

  






		 

		 


			 


    <tr><td colspan=2 class="spread"><a name="Biological Modeling"></a>Biological Modeling</td>
	  </tr>


     <a name="EvolutionaryComputationAndTheCValueParadox"></a><tr><td class="dte">Evolutionary Computation and the C-value Paradox</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/luke05evolutionary.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke. 2005. Evolutionary Computation and the C-value Paradox.  In <i>Proceedings of the 2005 Genetic and Evolutionary Computation Conference (GECCO)</i>.  Pages 91-97.  
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('f9');">Show / Hide</a><dd id='f9' style="display:none;">The C-value Paradox is the name given in biology to the wide variance in and often very large amount of DNA in eukaryotic genomes and the poor correlation between DNA length and perceived organism complexity. Several hypotheses exist which purport to explain the Paradox. Surprisingly there is a related phenomenon in evolutionary computation, known as code bloat, for which a different set of hypotheses has arisen. This paper describes a new hypothesis for the Cvalue Paradox derived from models of code bloat. The new explanation is that there is a selective bias in preference of genetic events which increase DNA material over those which decrease it. The paper suggests one possible concrete mechanism by which this may occur: deleting strands of DNA is more likely to damage genomic material than migrating or copying strands. The paper also discusses other hypotheses in biology and in evolutionary computation, and provides a simulation example as a proof of concept. 

       </dl>

  <a name="ThePerfectCElegansProjectAnInitialReport"></a><tr><td class="dte">The Perfect <i>C. elegans</i> Project: An Initial Report</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Hiroaki Kitano, Shugo Hamahashi, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <A HREF="https://cs.gmu.edu/~sean/papers/pce-alife.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Hiroaki Kitano, Shugo Hamahashi, and Sean Luke. 1998.  The perfect <i>C. elegans</i> project: an initial report.  In <i>Artificial Life</i>, 4:2 (Spring 1998), 141-156.  MIT Press.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c8');">Show / Hide</a><dd id='c8' style="display:none;">
	      The soil nematode <i>Caenorhabditis Elegans (C. elegans)</i> is the most investigated of all multi-cellular organisms. Since the proposal to use it as a model organism, a series of research projects have been undertaken, investigating a wide range of on various aspects of this organism. As a result, the complete cell lineage, neural circuitry, and various genes and their functions have been identified. The complete <i>C. elegans</i> DNA sequencing and gene expression mapping for each cell at different times during embryogenesis will be identified in a few years. Given the abundance of collected data, we believe that the time is ripe to introduce synthetic models of <i>C. elegans</i> to further enhance our understanding of the underlying principles of its development and behavior. For this reason, we have started the <i>Perfect C. elegans Project</i>, which aims to ultimately produce a complete synthetic model of <i>C. elegans</i> cellular structure and function. This paper describes the goal, the approach, and the initial results of the project. 
       </dl>
       
  <a name="BiologySeeItAgainForTheFirstTime"></a><tr><td class="dte">Biology: See It Again &mdash; for the First Time</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke, Shugo Hamahashi, Koji Kyoda, and Hiroki Ueda -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <A HREF="https://cs.gmu.edu/~sean/papers/biology.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Sean Luke, Shugo Hamahashi, Koji Kyoda, and Hiroki Ueda. 1998.  Biology: See It Again&mdash;for the First Time.  In <i>IEEE Intelligent Systems</i>.  13:5 (September/October 1998). 6-8.
	 <dt>
	      <i>First Paragraph: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d9a');">Show / Hide</a><dd id='d9a' style="display:none;">
		Computer science owes a huge debt to biological systems.  The field itself came about largely as an attempt to understand and replicate the function and abilities of the brain, a complex biological creation.  From this early lineage has sprung many subfields derived largely from biological metaphors: computer vision, neural networks, evolutionary computation, robotics, multi-agent studies, and much of artificial intelligence.  In some areas, the computer has bested its biological counterparts in efficiency and simplicity.  But for many domains, even after decades of hard work, the biological "real thing" is still superior to the artificial algorithms inspired by it.
       </dl>
       
		 
       


    <tr><td colspan=2 class="spread"><a name="KnowledgeRepresentation"></a>Knowledge Representation</td>
	  </tr>


                      <a name="SHOEABlueprintForTheSemanticWeb"></a><tr><td class="dte">SHOE: A Blueprint for the Semantic Web</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Jeff Heflin, James Hendler, and Sean Luke  -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/swbook03.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Jeff Heflin, James Hendler, and Sean Luke. 2003. SHOE: A Blueprint for the Semantic Web.  In Dieter Fensel, James Hendler, Henry Lieberman, and Wolfgang Wahlster, editors, <i>Spinning the Semantic Web: Bringing the World Wide Web to Its Full Potential</i>.  Pages 29-64.  MIT Press. 
	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('h0');">Show / Hide</a><dd id='h0' style="display:none;">The term <i>Semantic Web</i> was coined by Tim Berners-Lee to describe his proposal for a "web of meaning" as opposed to a "web of links" that currently exists on the Internet.  To achieve this vision, we need to develop languages and tools that enable machine understandable web pages.  The SHOE project, begun in 1995, was one of the first efforts to explore these issues.  In this paper, we describe our experiences developing and using the SHOE language.  We begin by describing the unique features of the World Wide Web and how they must influence potential Semantic Web languages.  Then we present SHOE, a language which allows web pages to be annotated with semantics, describe its syntax and semantics, and discuss our approaches to handling the problems of interoperability in distributed environments and ontology evolution.  Finally, we provide an overview of a suite of toolls for the Semantic Web, and discuss the application of the language and tools to two different domains.
        </dl>


        <a name="CopingWithChangingOntologiesInADistributedEnvironment"></a><tr><td class="dte">Coping with Changing Ontologies in a Distributed Environment</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Jeff Heflin, James Hendler, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/shoe-aaai99.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Jeff Heflin, James Hendler, and Sean Luke.  1999. Coping with Changing Ontologies in a Distributed Environment. <i>Ontology Management. Papers from the AAAI Workshop</i>. WS-99-13. AAAI Press. pp. 74-79.

	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c9');">Show / Hide</a><dd id='c9' style="display:none;">
We discuss the problems associated with versioning ontologies in distributed environments. This is an important issue because ontologies can be of great use in structuring and querying internet information, but many of the Internet's characteristics, such as distributed ownership, rapid evolution, and heterogeneity, make ontology management difficult. We present a scheme for classifying ontology revisions based upon the effect these changes would have on the data sources that reference the ontology. We also discuss how to manage these changes, especially when they are the result of integrating ontologies. Finally, we describe the simple elements of SHOE, a web-based knowledge representation language, that allow us to revise shared ontologies while maintaining consistency with web pages that already reference them.
       </dl>
       
 <a name="ApplyingOntologyToTheWeb:ACaseStudy"></a><tr><td class="dte">Applying Ontology to the Web: A Case Study</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Jeff Heflin, James Hendler, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/shoe-iwann99.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Jeff Heflin, James Hendler, and Sean Luke.  1999. Applying Ontology to the Web: A Case Study. In J. Mira, J. Sanchez-Andres (Eds.), <i>International Work-Conference on Artificial and Natural Neural Networks (IWANN)</i>. Vol 2. Springer, Berlin. pp. 715-724.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('c0');">Show / Hide</a><dd id='c0' style="display:none;">
This paper describes the use of Simple HTML Ontology Extensions (SHOE) in a real world internet application. SHOE allows authors to add semantic content to web pages and to relate this content to common ontologies that provide contextual information about the domain. Using this information, query systems can provide more accurate responses than are possible with the search engines available on the Web. We have applied these techniques to the domain of Transmissible Spongiform Encephalopathies (TSEs), a class of diseases that include "Mad Cow Disease". We discuss our experiences and provide lessons learned from the process.
       </dl>
       
  <a name="SHOEAKnowledgeRepresentationLanguageForInternetApplications"></a><tr><td class="dte">SHOE: A Knowledge Representation Language for Internet Applications</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Jeff Heflin, James Hendler, and Sean Luke -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/CS-TR-4078.pdf">PDF</a>
	 <dt><i>Note:</i><dd> A revised version of this paper formed Chapter 2 ("SHOE: A Blueprint for the Semantic Web") of the book <i>Spinning the Semantic Web</i>, edited by Dieter Fensel, James Hendler, Henry Lieberman, and Wolfgang Wahlster.  MIT Press.  2003.  ISBN 0-262-06232-1
	 <dt><i>Also Available As:</i><dd>UMCP-CSD Technical Report CS-TR-4078, or UMCP-UMIACS Technical Report UMIACS-TR-99-71, accessible online at the <a href="http://www.cs.umd.edu/Library/">Library</a> of the Department of Computer Science, University of Maryland at College Park.
	 <dt><i>Citation:</i><dd>Jeff Heflin, James Hendler, and Sean Luke. 1999. SHOE: A Knowledge Representation Language for Internet Applications.  Technical Report CS-TR-4078.  Department of Computer Science, University of Maryland at College Park.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d1a');">Show / Hide</a><dd id='d1a' style="display:none;">
	   It is our contention that the World Wide Web poses challenges to knowledge representation systems that fundamentally change the way we should design KR languages.  In this paper, we describe the simple HTML Ontology Extensions (SHOE), a KR language which allows web pages to be annotated with semantics.  We present a formalism for the language and discuss the features which make it well suited for the Web.  We describe the syntax and semantics of this language, and discuss the differences from traditional KR systems that make it more suited to modern web applications.  We also describe some generic tools for using the language and demonstrate its capabilities by describing two prototype systems that use it.  We also discuss some future tools currently being developed for the language.  The language, tools, and details of the applications are all available on the World Wide Web at <a href="http://www.cs.umd.edu/projects/plus/SHOE/">http://www.cs.umd.edu/projects/plus/SHOE/</a></dl>
       
       
  <a name="ReadingBetweenTheLinesUsingSHOEtoDiscoverImplicitKnowledgefromTheWeb"></a><tr><td class="dte">Reading Between the Lines: Using SHOE to Discover Implicit Knowledge from the Web</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Jeff Heflin, James Hendler, and Sean Luke -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/shoe-aaai98.pdf">PDF</a>
	 <dt><i>Citation: </i><dd>Jeff Heflin, James Hendler, and Sean Luke.  1998.  Reading between the lines: using SHOE to discover implicit knowledge from the web.  In <i>AI and Information Integration, Papers from the 1998 Workshop.</i> (WS-98-14).  AAAI Press.  51-57.
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d2');">Show / Hide</a><dd id='d2' style="display:none;">
	   This paper describes how SHOE, a set of Simple HTML Ontological Extensions, can be used to discover implicit knowledge from the World-Wide Wide Web (WWW).  SHOE allows authors to annotate their pages with ontology-based knowledge about page contents.  In previous papers, we discussed how the semantic knowledge provided by SHOE allows users to issue queries that are much more sophisticated than keyword search techniques, including queries that require retrieval of information from many sources.  Here, we expand upon this idea by describing how SHOE's ontologies allow agents to understand more than what is explicitly stated in Web pages through the use of context, inheritance an dinference.  We use examples to illustrate the usefulness of these features to Web agents and query engines.
       </dl>
       
       
  <a name="WebAgentsThatWork"></a><tr><td class="dte">Web Agents That Work</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>Sean Luke and James Hendler -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/multimedia2.pdf">PDF</a><br>
	 <dt><i>Note:</i><dd>This article covers similar issues as the paper, <a href="index.html#ontology-based-web-agents"><i>Ontology-based Web Agents</i></a>.
	 <dt><i>Citation:</i><dd>Sean Luke and James Hendler. 1997. Web Agents That Work.  In <i>IEEE Multimedia</i>. 4:3 (July-September 1997). IEEE Press.  76-80.
	      <dt><i>First Paragraph:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d0a');">Show / Hide</a><dd id='d0a' style="display:none;">
		There are two kinds of information-seekers currently wandering the World-Wide Web.  First there are us humans, the web-surfers for whom the Web was designed.  Second, there are increasing numbers of automated systems, <i>Web agents</i>, which gather information from the Web on our behalf.  At the present time, humans far outnumber web agents, but this could soon change: as the sheer volume of information on the Web increases,and the ratio of junk to useful information continues to grow, we will increasingly rely on agents to dig through all that muck to find our gems for us.
       </dl>
       
  <a name="ontology-based-web-agents"></a><tr><td class="dte">Ontology-based Web Agents</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd> Sean Luke, Lee Spector, David Rager, and James Hendler -->
	 <!--<dt><i>Formats:</i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/aa-paper.pdf">PDF</a>
	 <dt><i>Citation:</i><dd>Sean Luke, Lee Spector, David Rager, and James Hendler. 1997. Ontology-based Web Agents.  In <i>Proceedings of the First International Conference on Autonomous Agents (AutonomousAgents97)</i>. W. L. Johnson, ed.  New York: Association for Computing Machinery. 59-66.	 
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d3');">Show / Hide</a><dd id='d3' style="display:none;"> This paper describes <a href="http://www.cs.umd.edu/projects/plus/SHOE/">SHOE</a>, a set of Simple HTML Ontology Extensions which
	      allow World-Wide Web authors to annotate their pages with semantic
	      knowledge such as "I am a graduate student" or "This person is my graduate advisor".  These annotations are expressed in terms of ontological knowledge which can be generated by using or extending standard ontologies available on the Web.  This makes it possible to ask Web agent queries such as "Find me all graduate students in Maryland who are working on a project funded by DoD initiative 123-4567", instead of simplistic keyword searches enabled by current search engines.  We have also developed a web-crawling agent, Expos&eacute;e, which interns SHOE knowledge from web documents, making these kinds queries a reality.
       </dl>
       
       
         <a name="Ontology-BasedKnowledgeDiscoveryOnTheWorldWideWeb"></a><tr><td class="dte">Ontology-Based Knowledge Discovery on the World-Wide Web</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors:</i><dd>Sean Luke, Lee Spector, and David Rager -->
	<!--<dt><i>Formats:</i><dd>-->
	      <A HREF="https://cs.gmu.edu/~sean/papers/aaai-paper.pdf">PDF</a><br>
	      <!-- <A HREF="ontology/index.html">HTML</a> -->
	<dt><i>Note:</i><dd>This paper has been superceeded by <a href="index.html#ontology-based-web-agents">Ontology-based Web Agents</a>
	 <dt><i>Citation:</i><dd>Luke, Lee Spector, and David Rager. 1996. Ontology-Based Knowledge Discovery on the World-Wide Web.  In <i>Working Notes of the Workshop on Internet-Based Information Systems at the 13th National Conference on Artificial Intelligence (AAAI96)</i>. A. Franz and H. Kitano, eds.  AAAI.  96-102.	
	 <dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d4');">Show / Hide</a><dd id='d4' style="display:none;">
	   This paper describes <A HREF="http://www.cs.umd.edu/projects/plus/SHOE/">SHOE</a>, a set of Simple HTML Ontology Extensions. SHOE allows World-Wide Web authors to annotate their pages with ontology-based knowledge about page contents. We present examples showing how the use of SHOE can support a new generation of knowledge-based search and knowledge discovery tools that operate on the World-Wide Web.
       </dl>
 
       

       
  <a name="UsingTheParkaParallelKnowledgeRepresentationSystem"></a><tr><td class="dte"> Using the Parka Parallel Knowledge Representation System (Version 3.2)</td><td class="dde">
       <dl>
	 <!--<dt><i>Authors: </i><dd>William Anderson, Brian Kettler, Sean Luke, and James Hendler -->
	 <!--<dt><i>Formats: </i><dd>-->
	      <a href="https://cs.gmu.edu/~sean/papers/CS-TR-3485.pdf">PDF</a>
	 <dt><i>Also Available As:</i><dd>UMCP-CSD Technical Report CS-TR-3485, accessible online at the <a href="http://www.cs.umd.edu/Library/">Library</a> of the Department of Computer Science, University of Maryland at College Park.
	 <dt><i>Citation:</i><dd>William Anderson, Brian Kettler, Sean Luke, and James Hendler. 1995. Using the Parka Parallel Knowledge Representation System (Version 3.2).  Technical Report.  Department of Computer Science, University of Maryland at College Park.	
	 <dt><i>Abstract: </i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('d5');">Show / Hide</a><dd id='d5' style="display:none;">Parka is a symbolic, semantic network knowledge representation system that takes advantage of the massive parallelism of supercomputers such as the Connection Machine. The Parka language has many of the features of traditional semantic net/frame-based knowledge representation languages but also supports several kinds of rapid parallel inference mechanisms that scale to large knowledge-bases of hundreds of thousands of frames or more. Parka is intended for general-purpose use and has been used thus far to support A.I. systems for case-based reasoning and data mining. This document is a user manual for the current version of Parka, version 3.2. It describes the Parka language and presents some examples of knowledge representation using Parka. Details about the parallel algorithms, implementation, and empirical results are presented elsewhere.
       </dl>











       





	  <tr><td colspan=2 class="spread"><a name="Dissertation"></a>Dissertation</td>
	  </tr>


<a name="Fonts"></a><tr><td class="dtlast">Issues in Scaling Genetic Programming: Breeding Strategies, Tree Generation, and Code Bloat</td>

<td class="ddlast">
  <dl>
    <!--<dt><i>Author: </i><dd>Sean Luke -->
    <!--<dt><i>Formats:</i><dd>-->
      (<i>Recommended</i>) Double-Sided Version in <a href="https://cs.gmu.edu/~sean/papers/thesis2p.pdf">PDF</a><!-- (3.9 M, 178 pages)-->
     <br>
      Single-Sided Version in <a href="https://cs.gmu.edu/~sean/papers/thesis1p.pdf">PDF</a><!--(3.9 M, 165 pages)-->
<p>
    <dt><i>Citation:</i><dd>Sean Luke. 2000.  <i>Issues in Scaling Genetic Programming: Breeding Strategies, Tree Generation, and Code Bloat</i>.  Ph.D. Dissertation, Department of Computer Science, University of Maryland, College Park, Maryland.
<dt><i>Abstract:</i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="javascript:toggleLayer('a1');">Show / Hide</a><dd id='a1' style="display:none;"> Genetic Programming is an evolutionary computation technique which searches for those computer programs that best solve a given problem.  As genetic programming is applied to increasingly difficult problems, its effectiveness is hampered by the tendency of candidate program solutions to grow in size independent of any corresponding increases in quality.  This bloat in solutions slows the search process, interferes with genetic programming's searching, and ultimately consumes all available memory.  The challenge for scaling up genetic programming is to find the best solutions possible before bloat puts a stop to evolution.  This can be tackled either by finding better solutions more rapidly, or by taking measures to delay bloat as long as possible.
      
      <p style="margin-bottom: 0;">This thesis discusses issues both in speeding the search process and in delaying bloat in order to scale genetic programming to tackle harder problems.  It describes evolutionary computation and genetic programming, and details the application of genetic programming to cooperative robot soccer and to language induction.  The thesis then compares genetic programming breeding strategies, showing the conditions under which each strategy produces better individuals with less bloating.  It then analyzes the tree growth properties of the standard tree generation algorithms used, and proposes new, fast algorithms which give the user better control over tree size.  Lastly, it presents evidence which directly contradicts existing bloat theories, and gives a more general theory of code growth, showing that the issue is more complicated than it first appears.
</div>
<dt><i>Errata:</i><dd>
      <ol style="padding-bottom:0; margin-bottom:0; padding-top:0; margin-top:0;">
	<li>In Algorithm 2 (p. 6), the line <tt>P<-P\{q}</tt> should read <tt>P<-P\{s}</tt>
	<p class="smallskip"><li>Figures 5.2 through 5.5 (p. 38-39) are not in proper evolutionary-time order.  The proper order is 5.4, 5.5, 5.2, 5.3.</ol>
</dl>
      </td></tr>






              

                 

       
                        

       

  

       

     
  
         

 


     

       
                   

       

       	                   



       
       
      </table>
	<td width="30%" class="nav">
	 <a href="../sean.jpg"><img src="../sean.jpg" class="front" width="100%"></a>
	  <h1>Sean Luke</h1>
	  <div style="line-height: 150%">
	    <hr class="thick">
	  <a href="index.html" class="nav"><b>Publications</b></a>
	    <hr class="thin">
	  <a href="../research.1.html" class="nav">Code</a>
	  <hr class="thin">
	  <a href="../stuff.html" class="nav">Random Stuff</a>
	  <hr class="thin">
	  <a href="../index.html" class="nav">Home</a>
	  <hr class="thin">


	  <p class="bigskip"></p>
	  <hr class="thick">
	  <a href="http://cs.gmu.edu/~sean/papers/luke.bib" class="nav">BibTeX Entries</a>
	    <hr class="thin">
	  <a href="http://cs.gmu.edu/~sean/book/metaheuristics/" class="nav"><i>Essentials of Metaheuristics</i></a>
	    <hr class="thin">
	  <a href="index.html#Simulation" class="nav">Multiagent Behavior and Simulation</a>
	  <hr class="thin">
	  <a href="index.html#Multiagent" class="nav">Multiagent Learning and Fairness</a>
	  <hr class="thin">
	  <a href="index.html#EvolutionaryAlgorithms" class="nav">Evolutionary Algorithms</a>
	  <hr class="thin">
	  <a href="index.html#Coevolution" class="nav">Coevolution</a>
	  <hr class="thin">
	  <a href="index.html#GeneticProgramming" class="nav">Genetic Programming</a>
	  <hr class="thin">
	  <a href="index.html#CodeBloat" class="nav">Code Bloat</a>
	  <hr class="thin">
	  <a href="index.html#Robotics" class="nav">Robotics</a>
	  <hr class="thin">
	  <a href="index.html#Biological Modeling" class="nav">Biological Modeling</a>
	  <hr class="thin">
	  <a href="index.html#KnowledgeRepresentation" class="nav">Knowledge Representation</a>
	  <hr class="thin">
	  <a href="index.html#Dissertation" class="nav">Dissertation</a>
	  <hr class="thin">
	  </div>
      </table>
    </div>
    </body>
    </html>
